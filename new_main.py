# -*- coding: utf-8 -*-
"""
================================================================================
Investment Analysis Cloud Function (main.py)
================================================================================

Purpose:
--------
This script is a Google Cloud Function that acts as a secure and scalable API 
endpoint for analyzing investment documents. It is designed to process startup 
pitch decks, financial models, and other business documents to provide a 
comprehensive investment analysis, similar to what a venture capital analyst would produce.

Workflow:
---------
1.  **Receive Request:** The function listens for HTTPS POST requests containing
    one or more files (`multipart/form-data`). It also handles CORS preflight
    requests (`OPTIONS`).

2.  **Validate Files:** Each uploaded file is rigorously validated against a set
    of criteria:
    -   File presence, size (max 50MB), and type (PDF, DOCX, etc.).
    -   Content validity (e.g., is it a business document?).

3.  **Extract Text (Document AI):** The core text extraction is handled by Google
    Cloud Document AI. The script has intelligent logic to handle documents of
    any size:
    -   **Online Processing (Fast):** For smaller documents, it uses the fast,
        synchronous API.
        - With OCR (`USE_OCR=True`): Up to 15 pages.
        - Imageless (`USE_OCR=False`): Up to 30 pages.
    -   **Batch Processing (Robust):** If a document exceeds the online page limit,
        the script automatically uploads it to Google Cloud Storage (GCS) and
        initiates an asynchronous batch job, which can handle up to 500 pages.

4.  **Analyze Content (Vertex AI Gemini):** The extracted text is sent to a 
    powerful large language model (Gemini) via Vertex AI. The model is given a 
    detailed prompt instructing it to act as a senior VC analyst and return its
    findings in a structured JSON format.

5.  **Validate AI Response:** The JSON response from Gemini is validated to ensure
    it contains all the required fields (e.g., `startupName`, `recommendation`, 
    `riskAssessment`) before being sent to the client.

6.  **Return Structured Response:** The function returns a comprehensive JSON 
    object containing the full analysis, metadata about the processed files, 
    and detailed error messages if any step fails.

Key Technologies Used:
----------------------
-   **Google Cloud Functions:** For serverless, scalable execution.
-   **Google Cloud Document AI:** For high-accuracy text extraction with OCR.
-   **Google Cloud Vertex AI:** To access and run the Gemini generative model.
-   **Google Cloud Storage:** As a temporary holding area for large files during
    batch processing.
-   **Flask:** As the underlying web framework for handling HTTP requests.

How to Modify:
--------------
-   **Project & Processor IDs:** Change `PROJECT_ID`, `LOCATION`, `PROCESSOR_ID`,
    and bucket names under the `--- CONFIGURATION ---` section to match your
    Google Cloud setup.
-   **OCR Setting:** The `USE_OCR` variable is crucial. Set it to `"true"` (default)
    to analyze scanned documents or PDFs with text in images. Set it to `"false"`
    for faster processing of purely digital documents.
-   **Analysis Prompt:** The prompt within the `analyze_investment_opportunity`
    function can be modified to change the structure or focus of the AI's analysis.

================================================================================
INVESTMENT ANALYSIS CLOUD FUNCTION (main.py)
================================================================================

OVERVIEW:
---------
Google Cloud Function that analyzes startup pitch decks and business documents
using OCR (Document AI) and AI (Gemini) to provide venture capital investment
insights. Handles files up to 500 pages via intelligent online/batch processing.

PROCESSING FLOW:
----------------
1. Receive file(s) via multipart/form-data POST request
2. Validate files (type, size, extension)
3. Extract text using Document AI with OCR
   - Online: Up to 15 pages (fast)
   - Batch: Up to 500 pages (via GCS)
4. Validate extracted text (minimum length, business keywords)
5. Analyze with Gemini for VC-style investment analysis
6. Validate AI response structure
7. Return comprehensive JSON analysis

KEY FEATURES:
-------------
‚úì Handles multiple file uploads
‚úì OCR support for image-based PDFs
‚úì Automatic online-to-batch fallback
‚úì GCS cleanup after processing
‚úì Comprehensive error handling
‚úì CORS support for frontend integration


================================================================================
"""

# Import standard and Google Cloud libraries
import os    # For environment variables
import json  # For JSON encoding/decoding
import base64  # For encoding/decoding file data
import re  # For regex parsing
from flask import Request  # For HTTP request handling
from google.cloud import documentai, storage  # Document AI and GCS clients
import vertexai  # Vertex AI initialization
from vertexai.generative_models import GenerativeModel  # Gemini model
import functions_framework  # For GCP Cloud Functions
from datetime import datetime, timedelta  # For timestamps
from functools import wraps
from google.cloud import firestore
import uuid
# Below import are for dashboard generation
from reportlab.lib.pagesizes import letter, A4
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, PageBreak
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch
from reportlab.lib import colors
from reportlab.lib.enums import TA_CENTER, TA_LEFT, TA_RIGHT, TA_JUSTIFY
from io import BytesIO
import requests
from orchestrator import orchestrator
import firebase_admin
from firebase_admin import credentials, firestore

print("‚úÖ Orchestrator imported successfully")

try:
    firebase_admin.initialize_app()
    db = firestore.client()
    print("‚úÖ Firebase initialized")
except Exception as e:
    print(f"‚ö†Ô∏è Firebase init warning: {str(e)}")
    db = None

# --- CONFIGURATION ---
# Read project and processor info from environment or use default
PROJECT_ID = os.getenv("GCP_PROJECT_ID", "analyst-iq")
LOCATION = os.getenv("GCP_LOCATION", "us-central1")
PROCESSOR_ID = os.getenv("PROCESSOR_ID", "bd0934fc7b8dcd10")
PROCESSOR_LOCATION = os.getenv("PROCESSOR_LOCATION", "us")

INPUT_BUCKET_NAME = os.getenv("INPUT_BUCKET", "analyst-iq-docai-input")
OUTPUT_BUCKET_NAME = os.getenv("OUTPUT_BUCKET", "analyst-iq-docai-output")

# # ‚úÖ Initialize with explicit credentials
# try:
#     # Check if app already initialized
#     firebase_admin.get_app()
#     print("‚úÖ Firebase app already initialized")
# except ValueError:
#     # App not initialized, initialize it
#     firebase_admin.initialize_app(
#         options={
#             "projectId": "analyst-iq-5b9e1"
#         }
#     )
#     print("‚úÖ Firebase app initialized")

# # Get client
# db = firestore.client()
# print(f"‚úÖ Firestore client obtained")

# try:
#     test_doc = db.collection("_test").document("_test").get()
#     print(f"‚úÖ Firestore connection verified - Project: analyst-iq-5b9e1")
# except Exception as e:
#     print(f"‚ùå Firestore connection failed: {str(e)}")    


# File validation constants
ALLOWED_EXTENSIONS = ['pdf', 'txt', 'docx', 'pptx', 'doc', 'ppt']
ALLOWED_MIME_TYPES = [
    'application/pdf',
    'text/plain',
    'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
    'application/vnd.openxmlformats-officedocument.presentationml.presentation',
    'application/msword',
    'application/vnd.ms-powerpoint',
    'application/vnd.ms-office',
]
MAX_FILE_SIZE = 50 * 1024 * 1024  # 50MB
MIN_EXTRACTED_TEXT = 50  # Minimum characters for valid extraction (batch jobs may have small initial extracts)

# Document AI Processing limits
MAX_ONLINE_PAGES = 15  # OCR: 15 pages max for online/synchronous
MAX_IMAGELESS_PAGES = 30  # Imageless mode: 30 pages max
MAX_BATCH_PAGES = 500  # Batch: 500 pages max
USE_OCR = os.getenv("USE_OCR", "true").lower() == "true"  # Enable OCR by default
request_tracker = {} # ‚úÖ SECURITY: Rate limiting

# =============================================================================
# INITIALIZATION
# =============================================================================

# Initialize Google Cloud clients
try:
    vertexai.init(project=PROJECT_ID, location=LOCATION)
    docai_client = documentai.DocumentProcessorServiceClient(
        client_options={"api_endpoint": f"{PROCESSOR_LOCATION}-documentai.googleapis.com"}
    ) # Document AI
    storage_client = storage.Client()
    print(f"‚úÖ GCP clients initialized for project: {PROJECT_ID}")
except Exception as e:
    print(f"‚ö†Ô∏è Warning: GCP client initialization issue: {e}")

# =============================================================================
# HELPER FUNCTIONS
# =============================================================================

# Helper: Return CORS headers for HTTP responses
def get_cors_headers(include_content_type=True):
    """Return CORS headers for responses"""
    headers = {
        'Access-Control-Allow-Origin': '*',
        'Access-Control-Allow-Methods': 'GET, POST, OPTIONS',
        'Access-Control-Allow-Headers': 'Content-Type, Authorization',
        'Access-Control-Max-Age': '3600',
    }
    if include_content_type:
        headers['Content-Type'] = 'application/json'
    return headers

# ‚úÖ NEW: Text sanitization
def sanitize_extracted_text(text: str) -> str:
    """Remove potentially dangerous characters from extracted text"""
    if not text:
        return ""
    
    # Remove null bytes (very dangerous)
    text = text.replace('\x00', '')
    
    # Remove other control characters except newline, carriage return, tab
    # Control characters are ASCII 0-31, but we keep \n (10), \r (13), \t (9)
    text = ''.join(
        char for char in text 
        if ord(char) >= 32 or char in '\n\r\t'
    )
    
    # Remove excessive whitespace
    # This prevents weird spacing/newline injection
    text = ' '.join(text.split())
    
    return text    

# ‚úÖ NEW: Magic byte verification
def verify_file_magic_bytes(file_bytes: bytes, filename: str) -> tuple:
    """Verify file type using magic bytes (file signatures)"""
    magic_bytes_map = {
        'pdf': [b'%PDF'],
        'docx': [b'PK\x03\x04'],
        'xlsx': [b'PK\x03\x04'],
        'pptx': [b'PK\x03\x04'],
        'doc': [b'\xd0\xcf\x11\xe0\xa1\xb1\x1a\xe1'],
        'ppt': [b'\xd0\xcf\x11\xe0\xa1\xb1\x1a\xe1'],
    }
    
    ext = filename.rsplit('.', 1)[-1].lower() if '.' in filename else ''
    if ext not in magic_bytes_map:
        return False, "UNSUPPORTED_FORMAT", f"File type .{ext} not supported"
    
    file_start = file_bytes[:8]
    for signature in magic_bytes_map[ext]:
        if file_start.startswith(signature):
            return True, "VALID", "File signature verified"
    
    return False, "INVALID_FILE_SIGNATURE", f"File signature doesn't match .{ext} format"

# ‚úÖ NEW: Escape Gemini prompt input
def escape_gemini_input(text: str) -> str:
    """Escape special characters for Gemini"""
    text = text.replace('```', '` ` `')
    text = text.replace('---', '- - -')
    text = text.replace('{{', '{ {')
    return text

# ‚úÖ NEW: Rate limiting decorator
def rate_limit(max_requests=10, time_window=60):
    """Rate limit by IP address"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            client_ip = "unknown"
            try:
                if args and hasattr(args[0], 'remote_addr'):
                    client_ip = args[0].remote_addr
            except:
                pass
            
            now = datetime.now()
            if client_ip not in request_tracker:
                request_tracker[client_ip] = []
            
            request_tracker[client_ip] = [
                req_time for req_time in request_tracker[client_ip]
                if now - req_time < timedelta(seconds=time_window)
            ]
            
            if len(request_tracker[client_ip]) >= max_requests:
                cors_headers_local = get_cors_headers()
                return (
                    json.dumps({"status": "error", "code": "RATE_LIMITED", 
                               "message": "Too many requests. Please wait."}),
                    429,
                    cors_headers_local
                )
            
            request_tracker[client_ip].append(now)
            return func(*args, **kwargs)
        return wrapper
    return decorator

# Helper: Validate uploaded file for type, size, and extension
def validate_file_upload(file):
    """Validate uploaded file"""
    # Check file exists
    if not file or not file.filename:
        return False, "FILE_NOT_PROVIDED", "No file selected"
    
    # Check file size
    if file.content_length and file.content_length > MAX_FILE_SIZE:
        size_mb = file.content_length / (1024 * 1024)
        return False, "FILE_TOO_LARGE", f"File is {size_mb:.1f}MB. Maximum 50MB allowed"
    
    if file.content_length == 0:
        return False, "EMPTY_FILE", "File is empty"
    
    # Check file extension
    ext = file.filename.rsplit('.', 1)[-1].lower() if '.' in file.filename else ''
    if not ext or ext not in ALLOWED_EXTENSIONS:
        return False, "INVALID_FILE_TYPE", f"File type '.{ext}' not supported. Allowed: {', '.join(ALLOWED_EXTENSIONS)}"
    
    # Check MIME type (if provided)
    if file.content_type and file.content_type not in ALLOWED_MIME_TYPES:
        print(f"‚ö†Ô∏è MIME type {file.content_type} not in allowed list, but extension is valid")
    
    return True, "VALID", "File validation passed"

# ‚úÖ CONTENT-BASED VALIDATION (Not filename-based) - FORTRESS LEVEL
def validate_extraction(extracted_data, request_id=None):
    """
    Comprehensive multi-layer validation:
    1. Personal document detection
    2. Non-business content detection
    3. Business content verification
    4. Content structure analysis
    5. Optional Gemini secondary check
    """
    full_text = extracted_data.get('full_text', '')
    page_count = extracted_data.get('page_count', 0)

    # ‚úÖ SECURITY: Sanitize text - ADD THESE 2 LINES
    full_text = sanitize_extracted_text(full_text)
    extracted_data['full_text'] = full_text
    
    # Check if text was extracted
    if not full_text or len(full_text) < MIN_EXTRACTED_TEXT:
        return False, "NO_TEXT_EXTRACTED", "Unable to extract readable text from document. The file may be corrupted, image-only, encrypted, or not a valid business document."
    
    # Check page count
    if page_count == 0:
        return False, "NO_PAGES_DETECTED", "Document appears to be empty or unreadable."
    
    text_lower = full_text.lower()

    # ============================================================================
    # LAYER 0:CHECK BUISNESS CONTEXT FIRST
    # ============================================================================
    print(f"\n üìã LAYER 0: Business Context Pre-Check")
    
    business_context_keywords = [
        'startup', 'founder', 'investment', 'funding', 'pitch deck',
        'business model', 'revenue', 'market opportunity', 'team',
        'traction', 'ceo', 'co-founder', 'raise', 'series', 'seed',
        'investors', 'board', 'advisory', 'go-to-market', 'financial',
        'vet hospital', 'healthcare service', 'medical startup'  # ‚úÖ Added for Dr. Doodley
    ]
    
    business_context_score = sum(1 for kw in business_context_keywords if kw in text_lower)
    print(f"\n  üìã Business Context Keywords Found: {business_context_score}")
    
    if business_context_score >= 3:
        print(f"\n  LAYER 0 PASSED: This is clearly a BUSINESS document")
        print(f"\n  Skipping personal medical validation (irrelevant for business docs)")
        # Skip personal medical checks for business documents
        skip_personal_medical = True
    else:
        skip_personal_medical = False
        print(f"\n Low business context score - will perform full validation")
    
    # ============================================================================

    print(f"\n{'='*60}")
    print(f"üîí FORTRESS VALIDATION - 5 LAYER SECURITY CHECK")
    print(f"{'='*60}")
    
    # ============================================================================
    # LAYER 1: PERSONAL DOCUMENT DETECTION
    # ============================================================================
    print(f"\nüìã LAYER 1: Personal Document Detection")
    
    personal_indicators = {
        'identity_docs': {
            'keywords': ['passport', 'aadhar', 'pan number', 'ssn', 'social security', 
                        'driving license', 'voter id', 'national id', 'id card', 'license number',
                        'national identification', 'voter card', 'id proof'],
            'weight': 3.0  # High confidence indicator
        },
        'personal_medical': {
            'keywords': ['prescription', 'medical report', 'diagnosis', 'discharge summary',
                        'patient', 'physician', 'hospital', 'clinic', 'medical certificate',
                        'health record', 'patient id', 'doctor'],
            'weight': 2.5
        },
        'personal_financial': {
            'keywords': ['credit card', 'debit card', 'bank account', 'account number',
                        'swift code', 'ifsc', 'epfo', 'pf account', 'uan number',
                        'provident fund', 'bank statement', 'cheque', 'account holder'],
            'weight': 2.8
        },
        'personal_legal': {
            'keywords': ['birth certificate', 'marriage certificate', 'divorce', 'will', 'testament',
                        'adoption', 'death certificate', 'legal document'],
            'weight': 2.5
        },
        'resume_cv': {
            'keywords': ['objective:', 'work experience', 'skills section', 'education:', 'references',
                        'cv', 'resume', 'curriculum vitae', 'employment history', 'professional summary'],
            'weight': 3.5  # HIGHEST - clearest personal doc
        },
        'personal_photos': {
            'keywords': ['photo', 'photograph', 'selfie', 'portrait', 'headshot', 'image', 'picture'],
            'weight': 1.5
        },
    }
    
    personal_score = 0
    detected_personal_types = []
    
    for category, config in personal_indicators.items():
        # ‚úÖ NEW: Skip personal_medical if we detected business context
        if skip_personal_medical and category == 'personal_medical':
            print(f"   √¢‚Ä†' SKIPPING personal_medical (business document detected)")
            continue
            
        keywords = config['keywords']
        weight = config['weight']
        keyword_matches = sum(1 for kw in keywords if kw in text_lower)
        
        if keyword_matches >= 3:  # 2+ keywords in this category
            weighted_score = keyword_matches * weight
            personal_score += weighted_score
            detected_personal_types.append(f"{category}({keyword_matches})")
            print(f"   ‚ö†Ô∏è  Detected: {category} - {keyword_matches} keywords, Score: {weighted_score:.1f}")
    
    print(f"   üìä Personal Score: {personal_score:.1f}")
    
    if personal_score > 8:  # Stricter threshold
        error_msg = f"Document appears to be personal ({', '.join(set(detected_personal_types))})"
        print(f"   ‚ùå REJECTED: {error_msg}")
        extracted_data['validation_error_code'] = 'PERSONAL_DOCUMENT'
        if request_id:
            write_error_to_firestore(request_id, 'PERSONAL_DOCUMENT', "This document contains personal information and cannot be analyzed. Please upload business documents only.")
        return False, "PERSONAL_DOCUMENT", "This document contains personal information and cannot be analyzed. Please upload business documents only."
    
    resume_score = 0
    if text_lower.count('objective:') > 0:
        resume_score += 2
    if text_lower.count('work experience') > 0:
        resume_score += 2
    if text_lower.count('skills') > 0:
        resume_score += 1
    if text_lower.count('education') > 0:
        resume_score += 1

    if resume_score >= 4:
        # Flag as resume but DON'T reject yet
        # Let validate_file_set handle it
        extracted_data['document_classification'] = {
            'type': 'RESUME',
            'confidence': 'high',
            'reason': 'Resume detected'
        }
        print(f"   ‚ö†Ô∏è  Document appears to be a resume")


    # ============================================================================
    # LAYER 2: NON-BUSINESS CONTENT DETECTION
    # ============================================================================
    print(f"\nüìã LAYER 2: Non-Business Content Detection")
    
    non_business_indicators = {
        'recipe': {
            'keywords': ['recipe', 'ingredients', 'cooking', 'bake', 'fry', 'boil', 'simmer', 'instructions'],
            'weight': 2.5
        },
        'news_article': {
            'keywords': ['breaking news', 'news article', 'associated press', 'reuters', 'cnn', 'bbc', 
                        'news report', 'journalist', 'published', 'headline'],
            'weight': 2.0
        },
        'tutorial': {
            'keywords': ['tutorial', 'how to', 'step by step', 'beginner guide', 'for dummies', 'instructions'],
            'weight': 2.0
        },
        'entertainment': {
            'keywords': ['movie', 'film', 'actor', 'actress', 'screenplay', 'plot summary', 'netflix', 'imdb'],
            'weight': 2.0
        },
        'meme_cartoon': {
            'keywords': ['meme', 'comic', 'cartoon', 'joke', 'funny', 'laugh', 'hilarious'],
            'weight': 2.5
        },
    }
    
    non_business_score = 0
    detected_non_business_types = []
    
    for category, config in non_business_indicators.items():
        keywords = config['keywords']
        weight = config['weight']
        keyword_matches = sum(1 for kw in keywords if kw in text_lower)
        
        if keyword_matches >= 3:
            weighted_score = keyword_matches * weight
            non_business_score += weighted_score
            detected_non_business_types.append(f"{category}({keyword_matches})")
            print(f"   ‚ö†Ô∏è  Detected: {category} - {keyword_matches} keywords, Score: {weighted_score:.1f}")
    
    print(f"   üìä Non-Business Score: {non_business_score:.1f}")
    
    if non_business_score > 10:
        error_msg = f"Document contains non-business content ({', '.join(set(detected_non_business_types))})"
        print(f"   ‚ùå REJECTED: {error_msg}")
        if request_id:
            write_error_to_firestore(request_id, 'NOT_BUSINESS_CONTENT', "This document does not appear to be a business document. Please upload startup pitch decks, financial models, or business-related documents.")
        return False, "NOT_BUSINESS_CONTENT", "This document does not appear to be a business document. Please upload startup pitch decks, financial models, or business-related documents."
    
    # ============================================================================
    # LAYER 3: BUSINESS CONTENT VERIFICATION
    # ============================================================================
    print(f"\nüìã LAYER 3: Business Content Verification")
    
    business_indicators = {
        'pitch_deck': {
            'keywords': ['pitch', 'startup', 'investment opportunity', 'funding round', 'series a',
                        'seed round', 'venture capital', 'investor', 'raise', 'seed funding'],
            'weight': 1.5
        },
        'financial': {
            'keywords': ['revenue', 'profit', 'loss', 'balance sheet', 'cash flow', 'ebitda',
                        'financial projection', 'forecast', 'budget', 'burn rate', 'arr', 'mrr', 'expenses'],
            'weight': 1.3
        },
        'market_analysis': {
            'keywords': ['market size', 'tam', 'sam', 'sop', 'competitive analysis', 
                        'market share', 'competitor', 'industry analysis', 'market opportunity'],
            'weight': 1.2
        },
        'product': {
            'keywords': ['product', 'feature', 'technology', 'platform', 'software', 'service',
                        'solution', 'development', 'engineering', 'product roadmap'],
            'weight': 1.0
        },
        'team': {
            'keywords': ['founder', 'ceo', 'team', 'experience', 'background', 'expertise', 'advisor',
                        'management team', 'leadership'],
            'weight': 1.1
        },
        'traction': {
            'keywords': ['user', 'customer', 'growth', 'metric', 'kpi', 'retention', 'acquisition',
                        'traction', 'milestone', 'achievement', 'users', 'monthly active'],
            'weight': 1.0
        },
    }
    
    business_score = 0
    detected_business_categories = []
    
    for category, config in business_indicators.items():
        keywords = config['keywords']
        weight = config['weight']
        keyword_matches = sum(1 for kw in keywords if kw in text_lower)
        
        if keyword_matches >= 1:
            weighted_score = keyword_matches * weight
            business_score += weighted_score
            detected_business_categories.append(f"{category}({keyword_matches})")
            print(f"   ‚úÖ Detected: {category} - {keyword_matches} keywords, Score: {weighted_score:.1f}")
    
    print(f"   üìä Business Score: {business_score:.1f}")
    
    if business_score < 2:  # Require minimum business content
        print(f"   ‚ùå REJECTED: Insufficient business content (score: {business_score:.1f})")
        if request_id:
            write_error_to_firestore(request_id, 'INSUFFICIENT_BUSINESS_CONTENT', "Document does not contain enough business-related content. Please upload pitch decks, financial models, market research, or business documents.")
        return False, "INSUFFICIENT_BUSINESS_CONTENT", "Document does not contain enough business-related content. Please upload pitch decks, financial models, market research, or business documents."
    
    # ============================================================================
    # LAYER 4: PERSONAL vs BUSINESS RATIO CHECK
    # ============================================================================
    print(f"\nüìã LAYER 4: Personal vs Business Ratio Check")
    
    if business_score > 0:
        personal_business_ratio = personal_score / business_score
        print(f"   üìä Personal/Business Ratio: {personal_business_ratio:.2f}")
        
        if personal_business_ratio > 1.5 and personal_score > 5:  # Personal is >50% of business
            print(f"   ‚ùå REJECTED: Too much personal content mixed in")
            if request_id:
                write_error_to_firestore(request_id, 'MIXED_PERSONAL_BUSINESS', "Document contains a mix of personal and business content. Please upload pure business documents.")
            return False, "MIXED_PERSONAL_BUSINESS", "Document contains a mix of personal and business content. Please upload pure business documents."
    
    # ============================================================================
    # LAYER 5: CONTENT STRUCTURE ANALYSIS
    # ============================================================================
    print(f"\nüìã LAYER 5: Content Structure Analysis")
    
    # Check if document is too short
    word_count = len(full_text.split())
    print(f"   üìä Word Count: {word_count}")
    
    if word_count < 30:
        print(f"   ‚ùå REJECTED: Document too short")
        if request_id:
            write_error_to_firestore(request_id, 'INSUFFICIENT_CONTENT', "Document is too short to be a valid business document. Please upload documents with more detailed content.")
        return False, "INSUFFICIENT_CONTENT", "Document is too short to be a valid business document. Please upload documents with more detailed content."
    
    # Check for likely scanned ID (mostly short fields/numbers)
    lines = full_text.split('\n')
    very_short_lines = sum(1 for line in lines if len(line.strip()) < 20 and line.strip())
    short_line_ratio = very_short_lines / max(len(lines), 1)
    
    print(f"   üìä Short Lines Ratio: {short_line_ratio:.1%}")
    
    if short_line_ratio > 0.85:  # 70%+ very short lines
        print(f"   ‚ùå REJECTED: Likely structured form or ID")
        if request_id:
            write_error_to_firestore(request_id, 'LIKELY_STRUCTURED_DOCUMENT', "This appears to be a structured form or ID document, not a business document. Please upload proper business documents.")
        return False, "LIKELY_STRUCTURED_DOCUMENT", "This appears to be a structured form or ID document, not a business document. Please upload proper business documents."
    
    # ============================================================================
    # LAYER 6: OPTIONAL GEMINI SECONDARY CHECK (for edge cases)
    # ============================================================================
    # Only do secondary check if confidence is low (business_score between 4-6)
    
    if business_score < 3:
        print(f"\nüìã LAYER 6: Gemini Secondary Validation (Low Confidence)")
        
        try:
            is_business = gemini_document_type_check(full_text, business_score)
            
            if not is_business:
                print(f"   ‚ùå REJECTED: Gemini says not a business document")
                if request_id:
                    write_error_to_firestore(request_id, 'GEMINI_VALIDATION_FAILED', "AI analysis suggests this may not be a business document. Please verify and try again.")
                return False, "GEMINI_VALIDATION_FAILED", "AI analysis suggests this may not be a business document. Please verify and try again."
            else:
                print(f"   ‚úÖ PASSED: Gemini confirms business document")
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Gemini check failed (non-blocking): {str(e)}")
            # Continue anyway if Gemini fails
    
    # ============================================================================
    # FINAL DECISION
    # ============================================================================
    print(f"\n{'='*60}")
    print(f"‚úÖ VALIDATION PASSED - ALL LAYERS CLEARED")
    print(f"{'='*60}")
    print(f"üìä Final Scores:")
    print(f"   Personal: {personal_score:.1f}")
    print(f"   Business: {business_score:.1f}")
    print(f"   Non-Business: {non_business_score:.1f}")
    print(f"   Content Type: {', '.join(detected_business_categories)}")
    print(f"\nüîç DEBUG SCORES:")
    print(f"   personal_score: {personal_score:.1f} (threshold: > 8.0)")
    print(f"   business_score: {business_score:.1f} (threshold: < 2)")
    print(f"   non_business_score: {non_business_score:.1f} (threshold: > 10.0)")
    print(f"   short_line_ratio: {short_line_ratio:.1%} (threshold: > 85%)")
    
    return True, "VALID", "Content validation passed - Valid business document"


def extract_founders_from_analysis(analysis: dict) -> list:
    """Extract founder names from Gemini analysis"""
    try:
        founders = []
        
        # Get team experience from summary
        summary = analysis.get('summaryContent', {})
        team_exp = summary.get('teamExperience', '')
        
        if not team_exp:
            return []
        
        import re
        
        # Pattern 1: "Founder: Name"
        pattern1 = r'(?:Founder|CEO|Co-founder|CTO):\s*([A-Z][a-z]+\s+[A-Z][a-z]+)'
        matches1 = re.findall(pattern1, team_exp)
        founders.extend(matches1)
        
        # Pattern 2: "Name is the founder"
        pattern2 = r'([A-Z][a-z]+\s+[A-Z][a-z]+)\s+(?:is\s+)?(?:founder|ceo|cto|engineer)'
        matches2 = re.findall(pattern2, team_exp)
        founders.extend(matches2)
        
        # Remove duplicates and limit to 5
        founders = list(set(founders))[:5]
        
        print(f"   ‚úÖ Extracted {len(founders)} founder names: {founders}")
        return founders
        
    except Exception as e:
        print(f"   ‚ö†Ô∏è Error extracting founders: {str(e)}")
        return []

def fetch_founder_github_profiles(founder_names: list) -> dict:
    """Fetch founder GitHub profiles - COMPLETELY FREE"""
    try:
        print(f"\n   [1/4] GitHub - Fetching founder profiles...")
        
        github_profiles = []
        github_search_url = "https://api.github.com/search/users"
        
        if not founder_names or len(founder_names) == 0:
            print(f"   ‚ö†Ô∏è No founder names to search")
            return {'github_profiles': [], 'total_profiles': 0}
        
        for founder in founder_names:
            if not founder or len(founder.strip()) < 2:
                continue
            
            try:
                print(f"   üìç Searching for: {founder}")
                
                params = {
                    'q': founder,
                    'sort': 'followers',
                    'order': 'desc',
                    'per_page': 3
                }
                
                response = requests.get(github_search_url, params=params, timeout=10)
                
                if response.status_code == 200:
                    data = response.json()
                    
                    if data.get('total_count', 0) > 0:
                        # Get most likely match
                        user = data['items'][0]
                        
                        # Get detailed user info
                        user_detail_url = f"https://api.github.com/users/{user['login']}"
                        detail_response = requests.get(user_detail_url, timeout=10)
                        
                        if detail_response.status_code == 200:
                            details = detail_response.json()
                            
                            profile = {
                                'name': founder,
                                'github_username': user['login'],
                                'profile_url': user['html_url'],
                                'followers': user.get('followers', 0),
                                'public_repos': user.get('public_repos', 0),
                                'bio': user.get('bio', 'N/A'),
                                'company': details.get('company', 'N/A'),
                                'location': details.get('location', 'N/A'),
                                'blog': details.get('blog', 'N/A'),
                                'twitter': details.get('twitter_username', 'N/A'),
                                'public_gists': details.get('public_gists', 0),
                                'created_at': details.get('created_at', 'N/A')
                            }
                            
                            github_profiles.append(profile)
                            print(f"   ‚úÖ Found: @{user['login']} ({user.get('followers', 0)} followers)")
                    else:
                        print(f"   ‚ö†Ô∏è No GitHub profile found for: {founder}")
                
            except requests.exceptions.Timeout:
                print(f"   ‚ö†Ô∏è GitHub timeout for {founder}")
            except Exception as e:
                print(f"   ‚ö†Ô∏è GitHub error for {founder}: {str(e)[:50]}")
        
        print(f"   ‚úÖ GitHub: Found {len(github_profiles)} profiles")
        return {
            'github_profiles': github_profiles,
            'total_profiles': len(github_profiles),
            'confidence': 'high' if github_profiles else 'low'
        }
    
    except Exception as e:
        print(f"   ‚ùå GitHub fetch error: {str(e)}")
        return {'github_profiles': [], 'total_profiles': 0, 'error': str(e)}

def search_startup_comprehensively(startup_name: str, serpapi_key: str) -> dict:
    """Run 5 optimized searches for complete picture"""
    try:
        print(f"\n   [2/4] SerpAPI - Running 5 comprehensive searches...")
        
        if not serpapi_key:
            print(f"   ‚ö†Ô∏è SerpAPI key not found")
            return {}
        
        search_queries = [
            f"{startup_name} funding rounds",
            f"{startup_name} founders team",
            f"{startup_name} revenue metrics",
            f"{startup_name} series funding",
            f"{startup_name} investors"
        ]
        
        all_results = {
            'news': [],
            'funding_signals': {},
            'company_info': {}
        }
        
        search_count = 0
        
        for query in search_queries:
            if search_count >= 5:
                break
            
            try:
                print(f"   üîç Searching: {query}")
                
                params = {
                    'q': query,
                    'api_key': serpapi_key,
                    'engine': 'google',
                    'num': 5
                }
                
                response = requests.get(
                    "https://serpapi.com/search.json",
                    params=params,
                    timeout=15
                )
                
                if response.status_code == 200:
                    data = response.json()
                    
                    # Extract news results
                    news_results = data.get('news_results', [])
                    for news in news_results[:2]:
                        all_results['news'].append({
                            'title': news.get('title', ''),
                            'source': news.get('source', 'Google'),
                            'link': news.get('link', ''),
                            'date': news.get('date', ''),
                            'snippet': news.get('snippet', '')[:150]
                        })
                    
                    # Extract organic results
                    organic = data.get('organic_results', [])
                    for result in organic[:2]:
                        all_results['news'].append({
                            'title': result.get('title', ''),
                            'source': 'Google Search',
                            'link': result.get('link', ''),
                            'snippet': result.get('snippet', '')[:150]
                        })
                    
                    # Extract knowledge graph for company info (first search only)
                    if search_count == 0 and 'knowledge_graph' in data:
                        kg = data['knowledge_graph']
                        all_results['company_info'] = {
                            'name': kg.get('title', ''),
                            'description': kg.get('description', '')[:200],
                            'website': kg.get('website', ''),
                            'type': kg.get('type', '')
                        }
                    
                    print(f"   ‚úÖ Found {len(news_results)} news items")
                    search_count += 1
                    increment_api_usage('serpapi')
                
                elif response.status_code == 401:
                    print(f"   ‚ùå SerpAPI: Unauthorized (401) - Check API key")
                    break
                else:
                    print(f"   ‚ö†Ô∏è SerpAPI error: {response.status_code}")
                    
            except requests.exceptions.Timeout:
                print(f"   ‚ö†Ô∏è SerpAPI timeout on: {query}")
            except Exception as e:
                print(f"   ‚ö†Ô∏è SerpAPI error: {str(e)[:50]}")
        
        print(f"   ‚úÖ SerpAPI: Found {len(all_results['news'])} news items")
        return all_results
    
    except Exception as e:
        print(f"   ‚ùå SerpAPI comprehensive search error: {str(e)}")
        return {}



def classify_document_type(extracted_text: str, filename: str) -> dict:
    """
    Classify document as PRIMARY or SUPPLEMENTARY
    PRIMARY: Can be analyzed alone (pitch deck, financial model, business plan)
    SUPPLEMENTARY: Needs other documents (resume, team profiles)
    """
    
    text_lower = extracted_text.lower()
    
    # Check for RESUME
    resume_keywords = ['objective:', 'work experience', 'skills section', 'education:', 
                      'references', 'cv', 'resume', 'curriculum vitae', 'employment history']
    resume_matches = sum(1 for kw in resume_keywords if kw in text_lower)
    if resume_matches >= 2:
        return {
            "type": "RESUME",
            "is_primary": False,
            "description": "Resume/CV - Supplementary document"
        }
    
    # Check for PITCH DECK
    pitch_keywords = ['pitch', 'startup', 'investment opportunity', 'funding round', 
                     'series a', 'seed round', 'venture capital', 'investor', 'raise']
    pitch_matches = sum(1 for kw in pitch_keywords if kw in text_lower)
    if pitch_matches >= 2:
        return {
            "type": "PITCH_DECK",
            "is_primary": True,
            "description": "Pitch Deck - Primary document"
        }
    
    # Check for FINANCIAL MODEL
    financial_keywords = ['revenue', 'profit', 'cash flow', 'budget', 'burn rate',
                         'financial projection', 'forecast', 'balance sheet', 'ebitda']
    financial_matches = sum(1 for kw in financial_keywords if kw in text_lower)
    if financial_matches >= 3:
        return {
            "type": "FINANCIAL_MODEL",
            "is_primary": True,
            "description": "Financial Model - Primary document"
        }
    
    # Check for BUSINESS PLAN
    business_keywords = ['business model', 'market analysis', 'competitive analysis',
                        'executive summary', 'operations plan', 'go-to-market']
    business_matches = sum(1 for kw in business_keywords if kw in text_lower)
    if business_matches >= 2:
        return {
            "type": "BUSINESS_PLAN",
            "is_primary": True,
            "description": "Business Plan - Primary document"
        }
    
    # Check for MARKET RESEARCH
    market_keywords = ['market size', 'competitors', 'industry analysis', 'market share',
                      'competitive landscape', 'market opportunity']
    market_matches = sum(1 for kw in market_keywords if kw in text_lower)
    if market_matches >= 2:
        return {
            "type": "MARKET_RESEARCH",
            "is_primary": True,
            "description": "Market Research - Primary document"
        }
    
    # Default: Uncertain
    return {
        "type": "UNKNOWN",
        "is_primary": False,
        "description": "Document type unclear"
    }

# ‚úÖ NEW FUNCTION: Validate entire file set (not individual files)
def validate_file_set(all_extracted_data: list, file_names: list) -> tuple:
    """
    Validate entire set of uploaded files
    Returns: (is_valid, error_code, error_message)
    """
    
    if not all_extracted_data or len(all_extracted_data) == 0:
        return False, "NO_FILES", "No files to validate"
    
    has_primary_document = False
    has_supplementary_document = False
    document_types = []
    
    # Classify each file
    for i, extracted_data in enumerate(all_extracted_data):
        text = extracted_data.get('full_text', '')
        filename = file_names[i] if i < len(file_names) else f"file_{i}"
        
        doc_info = classify_document_type(text, filename)
        document_types.append(doc_info)
        
        if doc_info['is_primary']:
            has_primary_document = True
            print(f"   ‚úÖ {filename}: {doc_info['description']}")
        else:
            has_supplementary_document = True
            print(f"   ‚ö†Ô∏è  {filename}: {doc_info['description']}")
    
    # VALIDATION LOGIC
    
    # ‚ùå CASE 1: Only supplementary documents (e.g., resume alone)
    if has_supplementary_document and not has_primary_document:
        return False, "SUPPLEMENTARY_ONLY", \
            "Resume/Profile alone cannot be analyzed. Please upload a pitch deck, financial model, or business plan along with it."
    
    # ‚úÖ CASE 2: Has primary document (with or without supplementary)
    if has_primary_document:
        return True, "VALID", "File set validation passed"
    
    # ‚ùå CASE 3: Unknown documents
    return False, "UNKNOWN_DOCUMENTS", \
        "Could not identify business documents. Please upload a pitch deck or financial model."

# Helper: Validate Gemini AI response structure
def validate_gemini_response(analysis):
    """Validate Gemini response structure"""
    required_fields = {
        'startupName': str,
        'industry': str,
        'stage': str,
        'recommendation': dict,
        'keyMetrics': list,
        'riskAssessment': list,
        'summaryContent': dict,
    }
    
    # Check for missing top-level fields
    missing_fields = []
    for field, field_type in required_fields.items():
        if field not in analysis:
            missing_fields.append(field)
        elif not analysis[field]:
            missing_fields.append(field)
    
    if missing_fields:
        return False, "INCOMPLETE_ANALYSIS", f"Gemini response missing: {', '.join(missing_fields)}"
    
    # Perform deeper validation on critical fields like recommendation score
    rec = analysis.get('recommendation', {})
    if 'text' not in rec or 'score' not in rec:
        return False, "INVALID_RECOMMENDATION", "Recommendation missing text or score"
    
    score = rec.get('score')
    if not isinstance(score, (int, float)) or score < 0 or score > 100:
        return False, "INVALID_SCORE", f"Recommendation score must be 0-100, got {score}"
    
    # Validate summary content
    summary = analysis.get('summaryContent', {})
    if not summary.get('businessOverview'):
        return False, "MISSING_BUSINESS_OVERVIEW", "Business overview is empty"
    
    return True, "VALID", "Gemini response validation passed"


def update_firestore_progress(request_id: str, step: int, message: str, percent: int, status: str = "in_progress"):
    """Write progress to Firestore"""
    try:
        db.collection('analysis_progress').document(request_id).set({
            'step': step,
            'message': message,
            'percent': percent,
            'status': status,
            'timestamp': datetime.now().isoformat()
        }, merge=True)
        print(f"üìä Firestore: Step {step}/6 - {percent}%")
    except Exception as e:
        print(f"‚ö†Ô∏è  Firestore error: {str(e)}")


def write_error_to_firestore(request_id: str, error_code: str, error_message: str):
    """Write validation error to Firestore"""
    try:
        db.collection('analysis_progress').document(request_id).set({
            'step': 1,
            'status': 'failed',
            'error_code': error_code,
            'error_message': error_message,
            'timestamp': datetime.now().isoformat()
        })
        print(f"üìä Error logged to Firestore: {error_code}")
    except Exception as e:
        print(f"‚ö†Ô∏è  Firestore error write failed: {str(e)}")


def generate_call_prep_questions(analysis: dict) -> str:
    """Generate context-aware questions for founder call"""
    try:
        print(f"   ‚Üí Generating call prep questions...")
        print(f"   ‚Üí Analysis keys: {list(analysis.keys())}") 
        
        models_to_try = [
            "gemini-2.0-flash",
            "gemini-2.5-flash", 
            "gemini-1.5-flash",
            "gemini-2.5-pro",
            "gemini-1.5-pro",
        ]
        
        model = None
        model_used = None

        for model_name in models_to_try:
            try:
                print(f"   ‚Üí Testing {model_name}...")
                test_model = GenerativeModel(model_name)
                test_response = test_model.generate_content(
                    "test",
                    generation_config={"max_output_tokens": 2}
                )
                model = test_model
                model_used = model_name
                print(f"   ‚úÖ Using {model_name}")
                break
            except Exception as e:
                print(f"   ‚ö†Ô∏è  {model_name} unavailable: {str(e)[:100]}")
                continue
        
        if not model:
            raise Exception("No Gemini models available in this region")
        
        # Extract key info from analysis
        company = analysis.get('startupName', 'Company')
        summary = analysis.get('summaryContent', {})
        metrics = analysis.get('keyMetrics', [])
        risks = analysis.get('riskAssessment', [])
        traction = analysis.get('traction', {})
        
        # Build context
        context = f"""
Company: {company}
Business: {summary.get('businessOverview', '')[:200]}
Team: {summary.get('teamExperience', '')[:200]}
Key Metrics: {', '.join([m.get('label') for m in metrics[:5]])}
Top Risks: {', '.join([r.get('title') for r in risks[:3]])}
Traction: {traction}
"""
        
        prompt = f"""Based on this startup analysis, generate 7-10 specific, context-aware questions 
for a pre-investment founder call. These should probe deeper into:
- Claims made in the pitch deck
- Risks identified in analysis
- Growth metrics and unit economics
- Market understanding
- Team capabilities

Context:
{context}

Generate questions in this format:
1. [Question about X that references specific claim from deck]
2. [Question about Y that probes risk Z]
...

Each question should reference something specific from the analysis, not generic.
Start with "Pre-Call Founder Questions:" header."""

        print(f"   ‚Üí Sending to Gemini...")
        response = model.generate_content(
            prompt,
            generation_config={"max_output_tokens": 1024, "temperature": 0.7}
        )
        
        print(f"   ‚Üí Response received: {len(response.text)} chars") 
        
        questions = response.text if hasattr(response, 'text') else str(response)
        print(f"   ‚úì Generated {len(questions)} characters")
        return questions
        
    except Exception as e:
        print(f"   ‚úó Question generation error: {str(e)}")
        return "Questions could not be generated. Please review the analysis manually."


def add_benchmark_comparison(analysis: dict, sector: str) -> str:
    try:
        print(f"   ‚Üí Adding benchmark comparison...")
        
        # ‚úÖ TRY MULTIPLE MODELS (YOUR APPROACH)
        models_to_try = [
            "gemini-2.0-flash",
            "gemini-2.5-flash", 
            "gemini-1.5-flash",
            "gemini-2.5-pro",
            "gemini-1.5-pro",
        ]
        
        model = None
        model_used = None
        
        for model_name in models_to_try:
            try:
                print(f"   ‚Üí Testing {model_name}...")
                test_model = GenerativeModel(model_name)
                test_response = test_model.generate_content(
                    "test",
                    generation_config={"max_output_tokens": 2}
                )
                model = test_model
                model_used = model_name
                print(f"   ‚úÖ Using {model_name}")
                break
            except Exception as e:
                print(f"   ‚ö†Ô∏è  {model_name} unavailable: {str(e)[:100]}")
                continue
        
        if not model:
            raise Exception("No Gemini models available in this region")
        
        # Get metrics from analysis
        metrics = analysis.get('keyMetrics', [])
        market = analysis.get('marketOpportunity', {})
        stage = analysis.get('stage', 'Seed')
        traction = analysis.get('traction', {})
        
        metrics_text = "\n".join([f"- {m.get('label')}: {m.get('value')}" for m in metrics[:10]])
        
        sector_context = {
            'saas': 'SaaS B2B',
            'fintech': 'Financial Technology',
            'healthtech': 'Healthcare Tech',
            'edtech': 'Education Tech',
            'ai': 'Artificial Intelligence & Machine Learning',
            'ecommerce': 'E-Commerce & Direct-to-Consumer',
            'other': 'Technology'
        }
        
        context_label = sector_context.get(sector, 'Technology')
        
        prompt = f"""Compare these startup metrics against {context_label} industry benchmarks:

Stage: {stage}
Sector: {context_label}

Metrics:
{metrics_text}

Market: TAM={market.get('TAM', 'N/A')}, Growth={market.get('growthRate', 'N/A')}

Traction: 
- Revenue: {traction.get('revenue', 'N/A')}
- Users: {traction.get('users', 'N/A')}
- Customers: {traction.get('customers', 'N/A')}

For each metric, provide:
1. Industry Benchmark for {stage} stage {context_label} companies
2. Assessment: üü¢ Above Average | üü° Average | üî¥ Below Average
3. Implication for investment

Format as a structured comparison table."""

        response = model.generate_content(
            prompt,
            generation_config={"max_output_tokens": 1024, "temperature": 0.7}
        )
        
        benchmarking = response.text if hasattr(response, 'text') else str(response)
        print(f"   ‚úì Benchmarking analysis generated ({len(benchmarking)} chars)")
        return benchmarking
        
    except Exception as e:
        print(f"   ‚úó Benchmarking error: {str(e)}")
        import traceback
        traceback.print_exc()
        return "Benchmarking analysis unavailable."


# =============================================================================
# MAIN CLOUD FUNCTION
# =============================================================================

@rate_limit(max_requests=10, time_window=60)
@functions_framework.http
def analyze_document(request: Request):
    """Production Investment Analysis Cloud Function"""
    
    cors_headers = get_cors_headers()
    
    # Generate request ID
    request_id = str(uuid.uuid4())
    print(f"üîë Request ID: {request_id}")
    
    try:
        db.collection('analysis_progress').document(request_id).set({
            'step': 0,
            'message': 'Starting analysis...',
            'percent': 0,
            'status': 'in_progress',
            'timestamp': datetime.now().isoformat()
        })
    except:
        pass

    try:
        # Handle CORS preflight
        if request.method == 'OPTIONS':
            print("üîî CORS preflight request")
            headers = get_cors_headers(include_content_type=False)
            return ('', 204, headers)
        
        # Health check endpoint
        if request.args.get('health') == 'true':
            return health_check(request)
        
        print(f"\n{'='*80}")
        print(f"üöÄ STARTUP ANALYSIS REQUEST RECEIVED")
        print("="*80)
        print(f"üìÖ Timestamp: {datetime.now().isoformat()}")
        print(f"üìç Project: {PROJECT_ID} | Location: {LOCATION}")
        print(f"üìÑ Content-Type: {request.content_type}")
        
        # =====================================================================
        # VALIDATE REQUEST FORMAT
        # =====================================================================
        if not request.content_type or 'multipart/form-data' not in request.content_type:
            print(f"‚ùå Invalid request - must be multipart/form-data")
            error_result = {
                "status": "error",
                "error_type": "InvalidRequestError",
                "code": "INVALID_REQUEST_FORMAT",
                "message": "Request must be multipart/form-data with 'documents' field",
                "suggestion": "Ensure you're sending files as multipart/form-data with field name 'documents'",
                "timestamp": datetime.now().isoformat()
            }
            return (json.dumps(error_result), 400, cors_headers)
        
        # =====================================================================
        # GET FILES FROM REQUEST
        # =====================================================================
        files = request.files.getlist('documents')
        
        if not files:
            print("‚ùå No files found in 'documents' field")
            error_result = {
                "status": "error",
                "error_type": "MissingFilesError",
                "code": "FILE_NOT_PROVIDED",
                "message": "No files found. Submit files using 'documents' form field.",
                "suggestion": "Please select at least one PDF or document file to analyze",
                "timestamp": datetime.now().isoformat()
            }
            return (json.dumps(error_result), 400, cors_headers)
        
        print(f"üìÅ Found {len(files)} file(s): {[f.filename for f in files]}")
        
        # =====================================================================
        # PROCESS EACH FILE
        # =====================================================================
        file_names = []
        processed_files_info = []
        all_extracted_data = []
        validation_errors = []
        
        for file in files:
            if not file or not file.filename:
                print(f"‚ö†Ô∏è Skipping invalid file entry")
                continue
            
            print(f"\nüìÑ Processing: {file.filename}")
            
            # Read file content ONCE
            file_bytes = file.read()
            file_size = len(file_bytes)
            print(f"   Size: {file_size / 1024:.1f}KB")
            
            # Validate file size
            if file_size > MAX_FILE_SIZE:
                size_mb = file_size / (1024 * 1024)
                print(f"‚ùå File too large: {size_mb:.1f}MB (max {MAX_FILE_SIZE / (1024 * 1024):.0f}MB)")
                validation_errors.append({
                    "filename": file.filename,
                    "error": "FILE_TOO_LARGE",
                    "message": f"File is {size_mb:.1f}MB. Maximum 50MB allowed"
                })
                continue
            
            # Validate file is not empty
            if file_size == 0:
                print(f"‚ùå File is empty")
                validation_errors.append({
                    "filename": file.filename,
                    "error": "EMPTY_FILE",
                    "message": "File is empty"
                })
                continue
            
            # Validate file extension
            ext = file.filename.rsplit('.', 1)[-1].lower() if '.' in file.filename else ''
            if not ext or ext not in ALLOWED_EXTENSIONS:
                print(f"‚ùå Invalid file type: .{ext}")
                validation_errors.append({
                    "filename": file.filename,
                    "error": "INVALID_FILE_TYPE",
                    "message": f"File type '.{ext}' not supported. Allowed: {', '.join(ALLOWED_EXTENSIONS)}"
                })
                continue
            
            # Verify file magic bytes (security check)
            is_valid_signature, sig_code, sig_msg = verify_file_magic_bytes(file_bytes, file.filename)
            if not is_valid_signature:
                print(f"‚ùå Invalid file signature: {sig_msg}")
                validation_errors.append({
                    "filename": file.filename,
                    "error": sig_code,
                    "message": sig_msg
                })
                continue
            
            print(f"‚úÖ File validation passed")
            
            try:
                # Encode file bytes to base64
                file_content_base64 = base64.b64encode(file_bytes).decode('utf-8')
                
                # Extract text using Document AI
                print(f"   üî§ Sending to Document AI for processing...")
                extracted_data = extract_with_document_ai(file_content_base64, file.filename)
                
                if "error" in extracted_data:
                    print(f"‚ùå Extraction failed: {extracted_data.get('error')}")
                    validation_errors.append({
                        "filename": file.filename,
                        "error": "EXTRACTION_FAILED",
                        "message": extracted_data.get('error', 'Unknown extraction error')
                    })
                    continue
                
                # Validate extracted content
                is_valid, error_code, error_msg = validate_extraction(extracted_data, request_id)
                if not is_valid:
                    print(f"‚ùå Extraction validation failed: {error_msg}")
                    validation_errors.append({
                        "filename": file.filename,
                        "error": error_code,
                        "message": error_msg,
                        "document_type": extracted_data.get('document_classification', {}).get('type', 'UNKNOWN'),
                        "is_primary": extracted_data.get('document_classification', {}).get('is_primary', False)
                    })
                    continue
                
                # File processed successfully
                file_names.append(file.filename)
                processed_files_info.append({
                    "name": file.filename,
                    "type": get_document_type(file.filename),
                    "pages": extracted_data.get("page_count", 0),
                    "size": file_size,
                })
                
                extracted_data["file_name"] = file.filename
                all_extracted_data.append(extracted_data)
                print(f"‚úÖ File processed successfully: {extracted_data.get('page_count', 0)} pages")
                update_firestore_progress(request_id, 1, "Extracting text from documents", 15)
                
            except Exception as file_error:
                print(f"‚ùå Error processing file: {str(file_error)}")
                validation_errors.append({
                    "filename": file.filename,
                    "error": "PROCESSING_ERROR",
                    "message": str(file_error)
                })
                continue
        
        # =====================================================================
        # CHECK IF ANY FILES WERE SUCCESSFULLY PROCESSED
        # =====================================================================
        if not all_extracted_data:
            print(f"‚ùå No valid files could be processed")
            error_result = {
                "status": "error",
                "error_type": "ExtractionError",
                "code": "NO_VALID_FILES",
                "message": "No valid files could be processed",
                "details": validation_errors,
                "timestamp": datetime.now().isoformat()
            }
            return (json.dumps(error_result), 400, cors_headers)
        
        # =====================================================================
        # VALIDATE FILE SET
        # =====================================================================
        print(f"\nüìã VALIDATING FILE SET ({len(all_extracted_data)} files)")
        is_valid_set, set_code, set_msg = validate_file_set(all_extracted_data, file_names)
        
        if not is_valid_set:
            print(f"‚ùå File set validation failed: {set_msg}")
            if request_id:
                write_error_to_firestore(request_id, set_code, set_msg)
            error_result = {
                "status": "error",
                "error_type": "FileSetValidationError",
                "code": set_code,
                "message": set_msg,
                "timestamp": datetime.now().isoformat()
            }
            return (json.dumps(error_result), 400, cors_headers)
        
        print(f"‚úÖ File set validation passed")
        update_firestore_progress(request_id, 2, "Analyzing pitch deck content", 25)
        
        # =====================================================================
        # COMBINE MULTIPLE DOCUMENTS IF NEEDED
        # =====================================================================
        if len(all_extracted_data) > 1:
            print(f"\nüîö Combining {len(all_extracted_data)} documents for analysis")
            combined_text = "\n\n---DOCUMENT_BREAK---\n\n".join([
                f"[{data.get('file_name', 'Document')}]\n" + data.get("full_text", "")
                for data in all_extracted_data
            ])
            extracted_data = {
                "full_text": combined_text,
                "page_count": sum(d.get("page_count", 0) for d in all_extracted_data),
                "file_names": file_names,
                "processing_method": "multi_document"
            }
        else:
            extracted_data = all_extracted_data[0]
            

        # =====================================================================
        # TRANSCRIPT
        # =====================================================================    
        transcript_text = request.form.get('transcriptText', '')  # ‚úÖ NEW
        if transcript_text:
            print(f"\n‚úì Transcript received: {len(transcript_text)} chars")
            # APPEND transcript to extracted_data
            extracted_data['full_text'] = extracted_data.get('full_text', '') + "\n\n[FOUNDER TRANSCRIPT]\n" + transcript_text
            print(f"‚úì Combined text length: {len(extracted_data['full_text'])}")

        # =====================================================================    
        
        print(f"\n‚úÖ Extraction successful")
        print(f"   Pages: {extracted_data.get('page_count')}")
        print(f"   Text length: {len(extracted_data.get('full_text', ''))} characters")
        
        # =====================================================================
        # GET SECTOR PARAMETER
        # =====================================================================
        sector = request.form.get('sector') or request.form.get('category') or 'other'
        valid_sectors = ['saas', 'fintech', 'healthtech', 'edtech', 'ai', 'ecommerce', 'mobility', 'climate', 'consumer', 'other']
        if sector.lower() not in valid_sectors:
            sector = 'other'
        print(f"üè≠ Sector received: {sector}")
        
        # =====================================================================
        # EXTRACT FULL TEXT FROM extracted_data
        # =====================================================================
        all_extracted_text = extracted_data.get("full_text", "")
        if not all_extracted_text or len(all_extracted_text) < 50:
            print(f"‚ùå Extracted text is empty or too short")
            error_result = {
                "status": "error",
                "error_type": "ExtractionError",
                "code": "INSUFFICIENT_TEXT",
                "message": "Could not extract meaningful text from documents",
                "timestamp": datetime.now().isoformat()
            }
            return (json.dumps(error_result), 400, cors_headers)

        print(f"‚úÖ Extracted text ready: {len(all_extracted_text)} characters")


        # =====================================================================
        # NEW: CALL ORCHESTRATOR (All 4 Agents)
        # =====================================================================
        print(f"\nü§ñ CALLING ORCHESTRATOR (All 4 Agents)")
        update_firestore_progress(request_id, 3, "Running multi-agent analysis pipeline", 40)

        # ====================================================================
        # COLLECT OPTIONAL DATA (public_data + benchmarking)
        # ====================================================================
        
        #public_data = None
        #benchmarking = None
        
        # try:
        #     print(f"\nüåê Fetching public data...")
        #     startup_name_temp = extracted_data.get('startupName', 'Unknown') if isinstance(extracted_data, dict) else 'Unknown'
        #     public_data = synthesize_public_data_with_gemini(startup_name_temp, {})
        #     print(f"‚úÖ Public data fetched")
        # except Exception as e:
        #     print(f"‚ö†Ô∏è Public data failed (non-critical): {str(e)[:100]}")
        #     public_data = None
        
        # try:
        #     print(f"\nüìä Generating benchmarking...")
        #     benchmarking = add_benchmark_comparison({}, sector)
        #     print(f"‚úÖ Benchmarking generated")
        # except Exception as e:
        #     print(f"‚ö†Ô∏è Benchmarking failed (non-critical): {str(e)[:100]}")
        #     benchmarking = None
        
        # ====================================================================
        # CALL ORCHESTRATOR (All 4 Agents)
        # ====================================================================
        
        print(f"\nü§ñ Calling orchestrator.analyze_full_pipeline()...")
        
        user_id = request.headers.get('Authorization', '').replace('Bearer ', '')
        if not user_id:
            user_id = request.form.get('userId')  # Fallback

        print(f" User ID: {user_id}")    

        analysis = orchestrator.analyze_full_pipeline(
            extracted_text=all_extracted_text,
            file_names=file_names,
            sector=sector,
            user_id=user_id
        )
        

        print(f"\n[DEBUG] After Orchestrator:")
        print(f"  Analysis has call_prep_questions: {'call_prep_questions' in analysis}")
        print(f"  Length: {len(analysis.get('call_prep_questions', ''))}")
        # ====================================================================
        # CHECK FOR ORCHESTRATOR ERRORS
        # ====================================================================
        
        if analysis.get("status") != "success":
            print(f"‚ùå Orchestrator error: {analysis.get('error')}")
            error_result = {
                "status": "error",
                "error_type": "OrchestratorError",
                "code": "ORCHESTRATOR_FAILED",
                "message": f"Multi-agent analysis failed: {analysis.get('error')}",
                "stage": analysis.get('stage', 'unknown'),
                "timestamp": datetime.now().isoformat()
            }
            return (json.dumps(error_result), 500, cors_headers)
        
        print(f"‚úÖ Orchestrator analysis successful")
        print(f"   Company: {analysis.get('startupName', 'Unknown')}")
        print(f"   Recommendation: {analysis.get('recommendation', {}).get('text', 'Unknown')} ({analysis.get('recommendation', {}).get('score', 'N/A')}/100)")
        
        # =====================================================================
        # CREATE PAGE MAPS
        # =====================================================================
        page_maps = create_page_map_from_text(
            extracted_data.get("full_text", ""),
            extracted_data.get("file_name", "document.pdf")
        )

        # =====================================================================
        # üåê PUBLIC DATA
        # =====================================================================
        # try:
        #     print(f"\nüåê Fetching public data...")
        #     startup_name = analysis.get("startupName", "Unknown")
        #     public_data = synthesize_public_data_with_gemini(startup_name, analysis)
        #     print(f"‚úÖ Public data fetched")
        # except Exception as public_error:
        #     print(f"‚ö†Ô∏è Public data failed: {str(public_error)}")
        #     public_data = None  
        
        # =====================================================================
        # BUILD FINAL RESPONSE
        # =====================================================================
        model_used = analysis.get("ai_model_used", "gemini-1.5-flash")
        
        result = {
            "startupName": analysis.get("startupName", "Unknown Company"),
            "industry": analysis.get("industry", ""),
            "stage": analysis.get("stage", ""),
            "analysisDate": datetime.now().isoformat(),
            
            "recommendation": {
                "text": analysis.get("recommendation", {}).get("text", "REVIEW"),
                "score": int(analysis.get("recommendation", {}).get("score", 0)),
                "justification": analysis.get("recommendation", {}).get("justification", ""),
                "categoryScores": { 
                    "founder": analysis.get("recommendation", {}).get("founderScore", 75),
                    "market": analysis.get("recommendation", {}).get("marketScore", 75),
                    "differentiation": analysis.get("recommendation", {}).get("diffScore", 75),
                    "team": analysis.get("recommendation", {}).get("teamScore", 75)
                }
            },
            
            "keyMetrics": analysis.get("keyMetrics", []),
            "riskAssessment": analysis.get("riskAssessment", []),
            
            "summaryContent": {
                "businessOverview": analysis.get("summaryContent", {}).get("businessOverview", ""),
                "teamExperience": analysis.get("summaryContent", {}).get("teamExperience", ""),
                "productTech": analysis.get("summaryContent", {}).get("productTech", "")
            },
            
            "competitiveAnalysis": analysis.get("competitiveAnalysis", []),
            "marketOpportunity": analysis.get("marketOpportunity", {}),
            "financialProjections": analysis.get("financialProjections", []),
            "valuationInsights": analysis.get("valuationInsights", {}),
            "investmentTerms": analysis.get("investmentTerms", {}),
            "crossDocumentInsights": analysis.get("crossDocumentInsights", []),
            "traction": analysis.get("traction", {}),
            
            "page_maps": page_maps if page_maps else [],
            "documentsAnalyzed": processed_files_info if processed_files_info else [],
            "analysisMetadata": {
                "aiModel": model_used,
                "documentsProcessed": len(file_names),
                "analysisDepth": "comprehensive",
                "crossValidationPerformed": len(file_names) > 1,
                "processingTime": "real-time",
                "processingMethod": extracted_data.get("processing_method", "online_imageless")
            },
            "public_data": analysis.get("public_data", []),
            "benchmarking": analysis.get('benchmarking', ''),
            "memo_pdf_base64": analysis.get('memo_pdf_base64', None)
        }  

        print(f"\n‚úÖ Adding Question generated for founder to result")
        result['call_prep_questions'] = analysis.get('call_prep_questions', '')
        result['questions_json'] = analysis.get('questions_json', [])
        result['topics_to_explore'] = analysis.get('topics_to_explore', [])

        # print(f"\n‚úÖ Adding public_data to result")
        # result['public_data'] = public_data
        
        update_firestore_progress(request_id, 6, "Generating deal memo", 100)
        
        # # üìÑ PDF Generation with proper validation
        # print(f"\nüìÑ Generating investment memo...")
        # memo_pdf_bytes = None
        # memo_pdf_base64 = None
        # try:
        #     memo_pdf_bytes = generate_investment_memo_pdf(result)
            
        #     # ‚úÖ VALIDATE before encoding
        #     if memo_pdf_bytes and isinstance(memo_pdf_bytes, bytes) and len(memo_pdf_bytes) > 500:
        #         memo_pdf_base64 = base64.b64encode(memo_pdf_bytes).decode('utf-8')
        #         result['memo_pdf_base64'] = memo_pdf_base64
        #         print(f"‚úÖ PDF generated: {len(memo_pdf_base64)} chars")
        #     else:
        #         print(f"‚ùå PDF bytes invalid: {len(memo_pdf_bytes) if memo_pdf_bytes else 0}")
        #         result['memo_pdf_base64'] = None
                
        # except Exception as pdf_error:
        #     print(f"‚ùå PDF error: {str(pdf_error)}")
        #     import traceback
        #     traceback.print_exc()
        #     result['memo_pdf_base64'] = None

        # try:
        #     print(f"\n   ‚Üí Generating call prep questions...")
        #     call_prep_questions = generate_call_prep_questions(analysis)
        #     if call_prep_questions and len(call_prep_questions) > 50:
        #         result['call_prep_questions'] = call_prep_questions
        #         print(f"‚úì Call prep questions added ({len(call_prep_questions)} chars)")
        #     else:
        #         print(f"‚úó Call prep returned empty or too short: {call_prep_questions[:100]}")
        #         result['call_prep_questions'] = None
        # except Exception as e:
        #     print(f"‚úó Call prep error: {type(e).__name__}: {str(e)}")
        #     import traceback
        #     traceback.print_exc()  # ‚úÖ PRINT FULL STACK TRACE
        #     result['call_prep_questions'] = None

        # try:
        #     benchmarking = add_benchmark_comparison(analysis, sector)
        #     if benchmarking and len(benchmarking) > 50:
        #         result['benchmarking'] = benchmarking
        #         print(f"‚úì Benchmarking added ({len(benchmarking)} chars)")
        #     else:
        #         print(f"‚úó Benchmarking returned empty or too short: {benchmarking[:100]}")
        #         result['benchmarking'] = None
        # except Exception as e:
        #     print(f"‚úó Benchmarking error: {type(e).__name__}: {str(e)}")
        #     import traceback
        #     traceback.print_exc()  # ‚úÖ PRINT FULL STACK TRACE
        #     result['benchmarking'] = None


        # üåê PUBLIC DATA
        # try:
        #     print(f"\nüåê Fetching public data...")
        #     startup_name = analysis.get("startupName", "Unknown")
        #     public_data = synthesize_public_data_with_gemini(startup_name, analysis)
        #     result['public_data'] = public_data
        #     print(f"‚úÖ Public data fetched")
        # except Exception as public_error:
        #     print(f"‚ö†Ô∏è Public data failed: {str(public_error)}")
        #     result['public_data'] = None  

        try:
            db.collection('analysis_progress').document(request_id).update({
                'status': 'completed'
            })
        except:
            pass
        
        print(f"\n‚úÖ ANALYSIS COMPLETE")
        print("="*80)
        result['request_id'] = request_id

        return (json.dumps(result), 200, cors_headers)
    
    except Exception as e:
        print(f"\n‚ùå ERROR: {str(e)}")
        if 'request_id' in locals():
            write_error_to_firestore(request_id, 'INTERNAL_ERROR', str(e))
        import traceback
        traceback.print_exc()
        
        error_result = {
            "status": "error",
            "error_type": type(e).__name__,
            "code": "INTERNAL_ERROR",
            "message": str(e),
            "suggestion": "An unexpected error occurred. Please try again or contact support",
            "timestamp": datetime.now().isoformat()
        }
        print("="*80)
        return (json.dumps(error_result), 500, cors_headers)


# =============================================================================
# DOCUMENT EXTRACTION LOGIC
# =============================================================================


# This function is responsible for extracting text from a document using Document AI.
# It handles both small files (online processing) and large files (batch processing).
def extract_with_document_ai(pdf_content_base64: str, filename: str) -> dict:
    """Extract text from PDF using Document AI with OCR support and automatic batch fallback"""
  
   # The main logic is wrapped in a try/except block to catch errors, especially the page limit error.
    try:
        print(f"   ‚Üí Decoding PDF...")
        pdf_bytes = base64.b64decode(pdf_content_base64) # The file content is received as a Base64 encoded string. This line decodes it back into raw binary bytes.
        print(f"   ‚Üí Decoded size: {len(pdf_bytes)} bytes")
        
        print(f"\n{'='*60}")
        print(f"üìÑ EXTRACTING: {filename}")
        print(f"{'='*60}")

       # This constructs the full resource name for the Document AI processor, which is required by the API.
        # It uses the PROJECT_ID, PROCESSOR_LOCATION, and PROCESSOR_ID defined at the top of the script.
        name = docai_client.processor_path(PROJECT_ID, PROCESSOR_LOCATION, PROCESSOR_ID)
        # Log the processor name being used.
        print(f"   ‚Üí Processor: {name}")
        
        print(f"   ‚Üí Attempting ONLINE processing (max {MAX_ONLINE_PAGES} pages for OCR, {MAX_IMAGELESS_PAGES} for imageless)...")
        
      # Configure the OCR settings for the Document AI request.
        ocr_config = documentai.OcrConfig(
            # This is where the USE_OCR flag is used. It tells Document AI whether to perform OCR.
            #enable_ocr=USE_OCR,
            # This enables Document AI to parse the native structure of the PDF, which can improve layout understanding.
            enable_native_pdf_parsing=True,
            # If OCR is OFF (!USE_OCR), this tells Document AI to use the PDF's embedded digital text.
            #use_native_text=not USE_OCR,
            # Image quality scores are not needed for this use case, so it's disabled for efficiency.
            enable_image_quality_scores=False
        )
        
        # This creates the main request object to send to the Document AI API.
        request = documentai.ProcessRequest(
            # Specifies which processor to use.
            name=name,
            # Contains the actual document content.
            raw_document=documentai.RawDocument(
                # The binary content of the PDF file.
                content=pdf_bytes,
                # The MIME type of the file, telling the API it's a PDF.
                mime_type="application/pdf"
            ),
            # Attaches the processing options, including our OCR configuration.
            process_options=documentai.ProcessOptions(
                ocr_config=ocr_config
            ),
            # This tells Document AI not to wait for a human to review the document, ensuring a fully automated process.
            skip_human_review=True,
        )
       
        result = docai_client.process_document(request=request) # This is the actual API call to process the document synchronously (online).
        document = result.document  # The result object contains the processed document.

        page_count = len(document.pages) if document.pages else 0
        text_length = len(document.text or "")

        print(f"üìÑ EXTRACTION COMPLETE - DEBUG INFO:")
        print(f"   Total pages in PDF: {page_count}")
        print(f"   Text extracted: {len(document.text or '')} characters")
        print(f"   Average chars per page: {len(document.text or '') // max(page_count, 1)}")
        print(f"   Processing method used: ONLINE")
        print(f"   First 500 chars of extracted text:")
        print(f"   {(document.text or '')[:500]}")
        print(f"   Last 500 chars of extracted text:")
        print(f"   {(document.text or '')[-500:]}")
        extracted_text = document.text or ''
        if len(extracted_text) > 1000:
            print(f"   First 300 chars: {extracted_text[:300]}")
            print(f"   Last 300 chars: {extracted_text[-300:]}")
        else:
            print(f"   Full text: {extracted_text}")


        print(f"‚úÖ ONLINE processing SUCCESSFUL")
        print(f"   Pages extracted: {page_count}")
        print(f"   Text characters: {text_length}")
        print(f"   Avg chars per page: {text_length // max(page_count, 1)}")
        
        # Check for blank pages
        blank_pages = 0
        for i, page in enumerate(document.pages or []):
            if not page or len(str(page)) < 50:
                blank_pages += 1
        
        if blank_pages > 0:
            print(f"üìå Found {blank_pages} blank/minimal pages out of {page_count}")
        
        processing_mode = "OCR" if USE_OCR else "Imageless"
        print(f"   ‚úÖ Online processing successful ({processing_mode}): {len(document.pages)} pages")
        
       # Return a dictionary containing the extracted text, page count, and the processing method used.
        return {
            "full_text": document.text or "",
            "page_count": len(document.pages),
            "processing_method": f"online_{processing_mode.lower()}",
        }
    
    except Exception as e:
        error_str = str(e)
        if "PAGE_LIMIT_EXCEEDED" in error_str or "non-imageless" in error_str or "exceed" in error_str.lower():
            print(f"   ‚ö†Ô∏è Online processing failed - Document exceeds page limit")
            print(f"   ‚Üí Document has >15 pages (OCR) or >30 pages (imageless)")
            print(f"   ‚Üí Initiating BATCH processing for large document...")
            try:
                return batch_process_document(pdf_bytes, filename)
            except Exception as batch_error:
                print(f"   ‚ùå Batch processing also failed: {str(batch_error)}")
                raise batch_error
        else:
            print(f"   ‚ùå Document AI error: {error_str}")
            raise e


def batch_process_document(pdf_bytes: bytes, filename: str) -> dict:
    """Batch process large PDFs (>30 pages) using GCS - FIXED API"""
    try:
        from datetime import datetime as dt

        print(f"\n{'='*60}")
        print(f"üîÑ BATCH PROCESSING: {filename}")
        print(f"{'='*60}")
        
        timestamp = dt.now().strftime("%Y%m%d-%H%M%S%f")
        input_blob_name = f"input-{timestamp}.pdf"
        
        print(f"   ‚Üí Uploading to input bucket: {INPUT_BUCKET_NAME}")
        input_bucket = storage_client.bucket(INPUT_BUCKET_NAME)
        input_blob = input_bucket.blob(input_blob_name)
        input_blob.upload_from_string(pdf_bytes, content_type="application/pdf")
        print(f"   ‚úÖ Uploaded: {input_blob_name}")
        
        gcs_input_uri = f"gs://{INPUT_BUCKET_NAME}/{input_blob_name}"
        gcs_output_uri_prefix = f"output-{timestamp}"
        gcs_output_uri = f"gs://{OUTPUT_BUCKET_NAME}/{gcs_output_uri_prefix}/"
        
        print(f"   ‚Üí Submitting batch job to Document AI...")
        print(f"   Input: {gcs_input_uri}")
        print(f"   Output: {gcs_output_uri}")
        
        # ‚úÖ FIXED: Use correct API for current Document AI library
        # Old API: GcsOutputConfig (doesn't exist in current version)
        # New API: Use proper request structure
        
        name = docai_client.processor_path(PROJECT_ID, PROCESSOR_LOCATION, PROCESSOR_ID)
        
        # Configure batch input
        input_config = documentai.BatchDocumentsInputConfig(
            gcs_documents=documentai.GcsDocuments(
                documents=[
                    documentai.GcsDocument(
                        gcs_uri=gcs_input_uri,
                        mime_type="application/pdf"
                    )
                ]
            )
        )
        
        # ‚úÖ FIXED: Use correct output config format
        output_config = documentai.DocumentOutputConfig(
            gcs_output_config=documentai.DocumentOutputConfig.GcsOutputConfig(
                gcs_uri=gcs_output_uri
            )
        )
        
        request = documentai.BatchProcessRequest(
            name=name,
            input_documents=input_config,
            document_output_config=output_config,
        )
        
        print(f"   ‚Üí Batch job running (this may take a few minutes)...")
        operation = docai_client.batch_process_documents(request=request)
        
        # Wait for operation to complete
        print(f"   ‚Üí Waiting for batch processing to complete...")
        operation.result(timeout=900)  # Wait up to 15 minutes
        
        print(f"   ‚úÖ Batch job completed")
        
        # Retrieve results
        print(f"   ‚Üí Retrieving results from output bucket...")
        output_bucket = storage_client.bucket(OUTPUT_BUCKET_NAME)
        output_blobs = list(output_bucket.list_blobs(prefix=gcs_output_uri_prefix))
        
        if not output_blobs:
            raise Exception(f"No output files found in GCS. Check if bucket {OUTPUT_BUCKET_NAME} exists and batch job produced output.")
        
        print(f"   Found {len(output_blobs)} output files")
        
        # Find the JSON result file
        json_blobs = [b for b in output_blobs if b.name.endswith(".json")]

        if not json_blobs:
            raise Exception("Could not find any JSON output files from batch processing.")

        print(f"   Found {len(json_blobs)} JSON output file(s)")

        all_pages = []
        all_text = ""

        # Sort by blob name to read in order
        for json_blob in sorted(json_blobs, key=lambda b: b.name):
            print(f"   ‚Üí Reading {json_blob.name}...")
            json_string = json_blob.download_as_string()
            document = documentai.Document.from_json(json_string)
            
            # Collect pages and text from this file
            if document.pages:
                all_pages.extend(document.pages)
            if document.text:
                all_text += "\n" + document.text

        page_count = len(all_pages)
        extracted_text = all_text

        print(f"‚úÖ BATCH extraction SUCCESSFUL")
        print(f"   Total JSON files read: {len(json_blobs)}")
        print(f"   Pages extracted: {page_count}")
        print(f"   Text characters: {len(extracted_text)}")
                
        # Check for blank pages
        blank_pages = 0
        for i, page in enumerate(document.pages or []):
            if not page or len(str(page)) < 50:
                blank_pages += 1
        
        if blank_pages > 0:
            print(f"üìå Found {blank_pages} blank/minimal pages out of {page_count}")


        # Cleanup GCS files
        print(f"   ‚Üí Cleaning up GCS files...")
        try:
            input_blob.delete()
            for blob in output_blobs:
                blob.delete()
            print(f"   ‚úÖ Cleanup complete")
        except Exception as cleanup_error:
            print(f"   ‚ö†Ô∏è Warning: Cleanup failed (non-critical): {cleanup_error}")
        
        return {
            "full_text": extracted_text,
            "page_count": page_count,
            "processing_method": "batch_ocr",
        }
    
    except Exception as e:
        print(f"   ‚ùå Batch processing failed: {str(e)}")
        print(f"   Suggestions:")
        print(f"   1. Verify GCS buckets exist: {INPUT_BUCKET_NAME}, {OUTPUT_BUCKET_NAME}")
        print(f"   2. Check Cloud Function has Storage permissions")
        print(f"   3. Verify Document AI has access to GCS")
        raise e
# =============================================================================
# GEMINI ANALYSIS
# =============================================================================

# ‚úÖ NEW: Gemini Secondary Validation for Edge Cases
def gemini_document_type_check(text: str, business_score: float) -> bool:
    """
    Use Gemini to verify if document is actually a business document
    Only called when confidence is low (4-6 score range)
    """
    try:
        print(f"   ‚Üí Initializing Gemini for secondary check...")
        
        # Use fastest model available
        models_to_try = [
            "gemini-2.0-flash",
            "gemini-2.5-flash", 
            "gemini-1.5-flash",
            "gemini-2.5-pro",
            "gemini-1.5-pro",
        ]
        model = None
        
        for model_name in models_to_try:
            try:
                model = GenerativeModel(model_name)
                # Quick test
                model.generate_content("test", generation_config={"max_output_tokens": 2})
                print(f"   ‚Üí Using {model_name} for validation")
                break
            except:
                continue
        
        if not model:
            print(f"   ‚ö†Ô∏è  No Gemini models available for secondary check")
            return business_score >= 4  # Fall back to score threshold
        
        prompt = f"""You are a document classifier. Analyze this text ONLY.

Is this a BUSINESS DOCUMENT (pitch deck, financial model, market analysis, business plan, etc)?
Answer ONLY with: YES or NO

Text (first 2000 chars):
{text[:2000]}

Rules:
- YES: If document is about a startup, business, products, finances, market analysis, etc.
- NO: If document is personal (ID, resume, medical, news, recipe, entertainment, etc.)
- Answer ONLY: YES or NO (nothing else)
"""
        
        response = model.generate_content(
            prompt,
            generation_config={
                "temperature": 0.1,
                "max_output_tokens": 10,
            }
        )
        
        response_text = response.text.strip().upper() if hasattr(response, 'text') else ""
        
        print(f"   ‚Üí Gemini Response: {response_text}")
        
        if "YES" in response_text:
            return True
        else:
            return False
            
    except Exception as e:
        print(f"   ‚ö†Ô∏è  Gemini secondary check error: {str(e)}")
        return business_score >= 4  # Fall back to score threshold

# =============================================================================
# MAIN PDF GENERATION FUNCTION
# =============================================================================

def generate_investment_memo_pdf(analysis_result: dict) -> bytes:
    """Generate PROFESSIONAL PDF with charts and clean design"""
    try:
        from datetime import datetime
        from reportlab.lib.colors import HexColor
        
        print(f"\nüìÑ PDF BUILD START")
        
        if not analysis_result or not isinstance(analysis_result, dict):
            print(f"‚ùå Invalid input")
            return None
        
        buffer = BytesIO()
        doc = SimpleDocTemplate(
            buffer,
            pagesize=letter,
            topMargin=0.5*inch,
            bottomMargin=0.5*inch,
            leftMargin=0.6*inch,
            rightMargin=0.6*inch
        )
        
        elements = []
        styles = getSampleStyleSheet()
        
        # ============================================================
        # PROFESSIONAL STYLES
        # ============================================================
        
        # Colors
        primary_color = HexColor('#003366')      # Dark blue
        accent_color = HexColor('#00d4ff')       # Cyan
        success_color = HexColor('#22c55e')      # Green
        warning_color = HexColor('#f59e0b')      # Orange
        danger_color = HexColor('#ef4444')       # Red
        gray_bg = HexColor('#f8fafc')            # Light gray
        
        title_style = ParagraphStyle(
            'Title',
            parent=styles['Heading1'],
            fontSize=28,
            textColor=primary_color,
            spaceAfter=12,
            alignment=TA_CENTER,
            fontName='Helvetica-Bold',
            leading=32
        )
        
        subtitle_style = ParagraphStyle(
            'Subtitle',
            parent=styles['Normal'],
            fontSize=18,
            textColor=HexColor('#1e293b'),
            spaceAfter=8,
            alignment=TA_CENTER,
            fontName='Helvetica-Bold'
        )
        
        page_title = ParagraphStyle(
            'PageTitle',
            parent=styles['Heading1'],
            fontSize=16,
            textColor=primary_color,
            spaceAfter=14,
            spaceBefore=6,
            fontName='Helvetica-Bold',
        )
        
        section_heading = ParagraphStyle(
            'SectionHeading',
            parent=styles['Heading2'],
            fontSize=12,
            textColor=primary_color,
            spaceAfter=8,
            spaceBefore=12,
            fontName='Helvetica-Bold',
        )
        
        body_style = ParagraphStyle(
            'Body',
            parent=styles['Normal'],
            fontSize=10,
            textColor=HexColor('#334155'),
            alignment=TA_JUSTIFY,
            spaceAfter=10,
            leading=14,
            wordWrap='CJK'
        )
        
        highlight_style = ParagraphStyle(
            'Highlight',
            parent=styles['Normal'],
            fontSize=10,
            textColor=HexColor('#1e293b'),
            spaceAfter=8,
            leading=14,
            fontName='Helvetica-Bold'
        )
        
        # ============================================================
        # PAGE 1: PROFESSIONAL COVER
        # ============================================================
        print("   Page 1: Cover")
        elements.append(Spacer(1, 1.2*inch))
        
        # Title with accent line
        elements.append(Paragraph("INVESTMENT ANALYSIS", title_style))
        elements.append(Spacer(1, 0.05*inch))
        
        # Horizontal line
        from reportlab.platypus import HRFlowable
        hr = HRFlowable(width="60%", thickness=3, color=accent_color, spaceAfter=20, spaceBefore=10, hAlign='CENTER')
        elements.append(hr)
        
        startup = analysis_result.get('startupName', 'Unknown')
        elements.append(Paragraph(startup, subtitle_style))
        
        ind = analysis_result.get('industry', 'N/A')
        stg = analysis_result.get('stage', 'N/A')
        meta_text = ParagraphStyle('Meta', parent=styles['Normal'], fontSize=10, textColor=HexColor('#64748b'), alignment=TA_CENTER)
        elements.append(Paragraph(f"{ind} ‚Ä¢ {stg}", meta_text))
        
        elements.append(Spacer(1, 0.5*inch))
        
        # Recommendation Card
        rec = analysis_result.get('recommendation', {})
        score = int(rec.get('score', 50)) if rec.get('score') else 50
        text = rec.get('text', 'REVIEW')
        just = str(rec.get('justification', 'Analysis complete'))
        
        if text == 'INVEST':
            r_color, r_bg = success_color, HexColor('#d1fae5')
        elif text == 'PASS':
            r_color, r_bg = danger_color, HexColor('#fee2e2')
        else:
            r_color, r_bg = warning_color, HexColor('#fef3c7')
        
        # Score visualization
        score_card = [
            [
                Paragraph("<b>RECOMMENDATION</b>", ParagraphStyle('RecLabel', fontSize=10, textColor=HexColor('#64748b'))),
                Paragraph(f"<b><font size=16 color='{r_color}'>{text}</font></b>", styles['Normal']),
                Paragraph(f"<b><font size=14 color='{r_color}'>{score}/100</font></b>", styles['Normal'])
            ]
        ]
        
        score_tbl = Table(score_card, colWidths=[1.8*inch, 2.4*inch, 1.3*inch])
        score_tbl.setStyle(TableStyle([
            ('BACKGROUND', (0, 0), (-1, -1), r_bg),
            ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
            ('VALIGN', (0, 0), (-1, -1), 'MIDDLE'),
            ('ROUNDEDCORNERS', [10, 10, 10, 10]),
            ('LINEABOVE', (0, 0), (-1, 0), 2, r_color),
            ('PADDING', (0, 0), (-1, -1), 12),
            ('ROWHEIGHT', (0, 0), (-1, -1), 0.5*inch)
        ]))
        elements.append(score_tbl)
        
        elements.append(Spacer(1, 0.3*inch))
        
        # Justification
        just_style = ParagraphStyle(
            'Just', 
            parent=styles['Normal'], 
            fontSize=11, 
            alignment=TA_CENTER, 
            textColor=HexColor('#475569'),
            leading=16,
            wordWrap='CJK',
            leftIndent=50,
            rightIndent=50
        )
        elements.append(Paragraph(f'<i>"{just}"</i>', just_style))
        elements.append(PageBreak())
        
        # ============================================================
        # PAGE 2: EXECUTIVE SUMMARY
        # ============================================================
        print("   Page 2: Executive Summary")
        elements.append(Paragraph("EXECUTIVE SUMMARY", page_title))
        elements.append(Spacer(1, 0.1*inch))
        
        summ = analysis_result.get('summaryContent', {})
        
        elements.append(Paragraph("Business Overview", section_heading))
        elements.append(Paragraph(str(summ.get('businessOverview', 'N/A')), body_style))
        
        elements.append(Paragraph("Team & Experience", section_heading))
        elements.append(Paragraph(str(summ.get('teamExperience', 'N/A')), body_style))
        
        elements.append(Paragraph("Product & Technology", section_heading))
        elements.append(Paragraph(str(summ.get('productTech', 'N/A')), body_style))
        
        elements.append(PageBreak())
        
        # ============================================================
        # PAGE 3: KEY METRICS
        # ============================================================
        print("   Page 3: Key Metrics")
        elements.append(Paragraph("KEY INVESTMENT METRICS", page_title))
        elements.append(Spacer(1, 0.1*inch))
        
        metrics = analysis_result.get('keyMetrics', [])
        if metrics and len(metrics) > 0:
            m_data = [[
                Paragraph('<b>Metric</b>', ParagraphStyle('Th', fontSize=9, textColor=colors.white)),
                Paragraph('<b>Value</b>', ParagraphStyle('Th', fontSize=9, textColor=colors.white)),
                Paragraph('<b>Confidence</b>', ParagraphStyle('Th', fontSize=9, textColor=colors.white)),
                Paragraph('<b>Source</b>', ParagraphStyle('Th', fontSize=9, textColor=colors.white))
            ]]
            
            for m in metrics[:10]:
                src = m.get('source', {}) if isinstance(m.get('source'), dict) else {}
                conf = str(src.get('confidence', 'med')).upper()
                
                # Confidence color
                if conf == 'HIGH':
                    conf_color = success_color
                elif conf == 'MEDIUM':
                    conf_color = warning_color
                else:
                    conf_color = HexColor('#94a3b8')
                
                m_data.append([
                    Paragraph(str(m.get('label', ''))[:50], body_style),
                    Paragraph(f"<b>{str(m.get('value', 'N/A'))[:40]}</b>", highlight_style),
                    Paragraph(f'<font color="{conf_color}"><b>{conf}</b></font>', body_style),
                    Paragraph(str(src.get('location', 'N/A'))[:35], ParagraphStyle('Small', fontSize=8, textColor=HexColor('#64748b')))
                ])
            
            m_tbl = Table(m_data, colWidths=[1.6*inch, 1.3*inch, 0.9*inch, 1.4*inch])
            m_tbl.setStyle(TableStyle([
                ('BACKGROUND', (0, 0), (-1, 0), primary_color),
                ('TEXTCOLOR', (0, 0), (-1, 0), colors.white),
                ('ALIGN', (0, 0), (-1, -1), 'LEFT'),
                ('VALIGN', (0, 0), (-1, -1), 'TOP'),
                ('FONTSIZE', (0, 0), (-1, -1), 9),
                ('GRID', (0, 0), (-1, -1), 0.5, HexColor('#e2e8f0')),
                ('ROWBACKGROUNDS', (0, 1), (-1, -1), [colors.white, gray_bg]),
                ('PADDING', (0, 0), (-1, -1), 8),
                ('ROWHEIGHT', (0, 0), (-1, 0), 0.3*inch),
            ]))
            elements.append(m_tbl)
        
        elements.append(PageBreak())
        
        # ============================================================
        # PAGE 4: MARKET OPPORTUNITY
        # ============================================================
        print("   Page 4: Market Opportunity")
        elements.append(Paragraph("MARKET OPPORTUNITY", page_title))
        elements.append(Spacer(1, 0.1*inch))
        
        market = analysis_result.get('marketOpportunity', {})
        
        # Market sizing cards
        market_data = [
            ['TAM', str(market.get('TAM', 'N/A'))],
            ['SAM', str(market.get('SAM', 'N/A'))],
            ['SOM', str(market.get('SOM', 'N/A'))],
            ['Growth Rate', str(market.get('growthRate', 'N/A'))]
        ]
        
        for label, value in market_data:
            card = [[
                Paragraph(f'<b>{label}</b>', ParagraphStyle('Label', fontSize=9, textColor=HexColor('#64748b'))),
                Paragraph(value, highlight_style)
            ]]
            card_tbl = Table(card, colWidths=[1.2*inch, 4.0*inch])
            card_tbl.setStyle(TableStyle([
                ('BACKGROUND', (0, 0), (-1, -1), gray_bg),
                ('ALIGN', (0, 0), (0, -1), 'LEFT'),
                ('ALIGN', (1, 0), (1, -1), 'LEFT'),
                ('VALIGN', (0, 0), (-1, -1), 'MIDDLE'),
                ('PADDING', (0, 0), (-1, -1), 10),
                ('ROWHEIGHT', (0, 0), (-1, -1), 0.35*inch),
                ('LINEBELOW', (0, 0), (-1, -1), 1, HexColor('#e2e8f0'))
            ]))
            elements.append(card_tbl)
            elements.append(Spacer(1, 0.08*inch))
        
        elements.append(Spacer(1, 0.1*inch))
        elements.append(Paragraph("Market Trends", section_heading))
        elements.append(Paragraph(str(market.get('marketTrends', 'N/A')), body_style))
        
        elements.append(Spacer(1, 0.1*inch))
        elements.append(Paragraph("Competitive Landscape", section_heading))
        
        comp = analysis_result.get('competitiveAnalysis', [])
        if comp and len(comp) > 0:
            for idx, c in enumerate(comp[:5], 1):
                elements.append(Paragraph(
                    f'<b>{idx}. {c.get("competitor", "N/A")}</b> - {c.get("differentiators", "N/A")[:120]}',
                    body_style
                ))
        
        elements.append(PageBreak())
        
        # ============================================================
        # PAGE 5: TRACTION & FINANCIALS (WITH CHART)
        # ============================================================
        print("   Page 5: Traction & Financials")
        elements.append(Paragraph("TRACTION & GROWTH", page_title))
        elements.append(Spacer(1, 0.1*inch))
        
        trac = analysis_result.get('traction', {})
        if trac and any(trac.values()):
            traction_items = [
                ('customers', 'üë• Customers'),
                ('revenue', 'üí∞ Revenue'),
                ('users', 'üë§ Users'),
                ('growth_rate', 'üìà Growth Rate'),
                ('partnerships', 'ü§ù Partnerships')
            ]
            
            for key, label in traction_items:
                if trac.get(key) and str(trac.get(key)) not in ['Not mentioned in document', 'N/A']:
                    elements.append(Paragraph(f'<b>{label}</b>', section_heading))
                    elements.append(Paragraph(str(trac.get(key)), body_style))
        
        elements.append(Spacer(1, 0.15*inch))
        elements.append(Paragraph("FINANCIAL PROJECTIONS", page_title))
        elements.append(Spacer(1, 0.1*inch))
        
        fin = analysis_result.get('financialProjections', [])
        if fin and len(fin) > 0:
            # Add revenue chart (calls top-level helper)
            revenue_chart = create_revenue_chart(fin)
            if revenue_chart:
                elements.append(revenue_chart)
                elements.append(Spacer(1, 0.15*inch))
            
            # Financial table
            f_data = [[
                Paragraph('<b>Year</b>', ParagraphStyle('Th', fontSize=9, textColor=colors.white)),
                Paragraph('<b>Revenue</b>', ParagraphStyle('Th', fontSize=9, textColor=colors.white)),
                Paragraph('<b>Margins</b>', ParagraphStyle('Th', fontSize=9, textColor=colors.white))
            ]]
            
            for f in fin[:5]:
                f_data.append([
                    Paragraph(str(f.get('year', '')), body_style),
                    Paragraph(f"<b>{str(f.get('revenue', 'N/A'))[:25]}</b>", highlight_style),
                    Paragraph(str(f.get('margins', 'N/A'))[:25], body_style)
                ])
            
            f_tbl = Table(f_data, colWidths=[1.2*inch, 2.0*inch, 2.0*inch])
            f_tbl.setStyle(TableStyle([
                ('BACKGROUND', (0, 0), (-1, 0), primary_color),
                ('TEXTCOLOR', (0, 0), (-1, 0), colors.white),
                ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
                ('VALIGN', (0, 0), (-1, -1), 'MIDDLE'),
                ('GRID', (0, 0), (-1, -1), 0.5, HexColor('#e2e8f0')),
                ('ROWBACKGROUNDS', (0, 1), (-1, -1), [colors.white, gray_bg]),
                ('PADDING', (0, 0), (-1, -1), 8),
            ]))
            elements.append(f_tbl)
        
        elements.append(PageBreak())
        
        # ============================================================
        # PAGE 6: VALUATION & INVESTMENT TERMS
        # ============================================================
        print("   Page 6: Valuation & Investment Terms")
        elements.append(Paragraph("VALUATION & INVESTMENT TERMS", page_title))
        elements.append(Spacer(1, 0.1*inch))
        
        val = analysis_result.get('valuationInsights', {})
        term = analysis_result.get('investmentTerms', {})
        
        has_valuation_data = val and any(v for v in val.values() if v and str(v) not in ['Not mentioned in document', 'N/A', ''])
        has_terms_data = term and any(v for v in term.values() if v and str(v) not in ['Not mentioned in document', 'N/A', ''])
        
        if has_valuation_data or has_terms_data:
            # VALUATION SECTION
            if has_valuation_data:
                elements.append(Paragraph("üí∞ Valuation", section_heading))
                
                valuation_items = [
                    ('currentValuation', 'Current Valuation'),
                    ('post_moneyValuation', 'Post-Money Valuation'),
                    ('pricingPerShare', 'Price Per Share'),
                    ('comparableCompanies', 'Comparable Companies'),
                    ('keyMetricsForValuation', 'Key Valuation Metrics')
                ]
                
                for key, label in valuation_items:
                    value = val.get(key)
                    if value and str(value) not in ['Not mentioned in document', 'N/A', '']:
                        card = [[
                            Paragraph(f'<b>{label}</b>', ParagraphStyle('Label', fontSize=9, textColor=HexColor('#64748b'))),
                            Paragraph(str(value), highlight_style)
                        ]]
                        card_tbl = Table(card, colWidths=[1.8*inch, 3.4*inch])
                        card_tbl.setStyle(TableStyle([
                            ('BACKGROUND', (0, 0), (-1, -1), HexColor('#fef3c7')),
                            ('ALIGN', (0, 0), (0, -1), 'LEFT'),
                            ('ALIGN', (1, 0), (1, -1), 'LEFT'),
                            ('VALIGN', (0, 0), (-1, -1), 'MIDDLE'),
                            ('PADDING', (0, 0), (-1, -1), 10),
                            ('LINEBELOW', (0, 0), (-1, -1), 1, HexColor('#fde68a'))
                        ]))
                        elements.append(card_tbl)
                        elements.append(Spacer(1, 0.08*inch))
                
                elements.append(Spacer(1, 0.15*inch))
            
            # INVESTMENT TERMS SECTION
            if has_terms_data:
                elements.append(Paragraph("üìã Investment Terms", section_heading))
                
                terms_items = [
                    ('roundType', 'Round Type', 'üéØ'),
                    ('requestedAmount', 'Amount Requested', 'üíµ'),
                    ('equity', 'Equity Offered', 'üìä'),
                    ('minimumInvestment', 'Minimum Investment', 'üí≥'),
                    ('useOfFunds', 'Use of Funds', 'üìà'),
                    ('fundingTimeline', 'Timeline', '‚è∞')
                ]
                
                for key, label, icon in terms_items:
                    value = term.get(key)
                    if value and str(value) not in ['Not mentioned in document', 'N/A', '']:
                        card = [[
                            Paragraph(f'<b>{icon} {label}</b>', ParagraphStyle('Label', fontSize=9, textColor=HexColor('#64748b'))),
                            Paragraph(str(value), highlight_style)
                        ]]
                        card_tbl = Table(card, colWidths=[1.8*inch, 3.4*inch])
                        card_tbl.setStyle(TableStyle([
                            ('BACKGROUND', (0, 0), (-1, -1), HexColor('#dbeafe')),
                            ('ALIGN', (0, 0), (0, -1), 'LEFT'),
                            ('ALIGN', (1, 0), (1, -1), 'LEFT'),
                            ('VALIGN', (0, 0), (-1, -1), 'MIDDLE'),
                            ('PADDING', (0, 0), (-1, -1), 10),
                            ('LINEBELOW', (0, 0), (-1, -1), 1, HexColor('#bfdbfe'))
                        ]))
                        elements.append(card_tbl)
                        elements.append(Spacer(1, 0.08*inch))
        else:
            # No valuation/terms data
            elements.append(Paragraph(
                "<i>No valuation or investment terms data available in the provided documents.</i>",
                ParagraphStyle('NoData', fontSize=10, textColor=HexColor('#94a3b8'), alignment=TA_CENTER)
            ))
        
        elements.append(PageBreak())
        
        # ============================================================
        # PAGE 7: RISK ASSESSMENT (WITH PIE CHART)
        # ============================================================
        print("   Page 7: Risk Assessment")
        elements.append(Paragraph("RISK ASSESSMENT", page_title))
        elements.append(Spacer(1, 0.1*inch))
        
        risks = analysis_result.get('riskAssessment', [])
        if risks and len(risks) > 0:
            # Add risk pie chart (calls top-level helper)
            risk_chart = create_risk_pie_chart(risks)
            if risk_chart:
                elements.append(risk_chart)
                elements.append(Spacer(1, 0.15*inch))
            
            for idx, risk in enumerate(risks[:5], 1):
                level = str(risk.get('level', 'medium')).upper()
                title = str(risk.get('title', 'Risk'))
                desc = str(risk.get('description', 'N/A'))
                mitigation = str(risk.get('mitigation', 'N/A'))
                impact = str(risk.get('impact', 'N/A'))
                
                if level == 'HIGH':
                    r_color = danger_color
                    r_bg = HexColor('#fee2e2')
                elif level == 'MEDIUM':
                    r_color = warning_color
                    r_bg = HexColor('#fef3c7')
                else:
                    r_color = success_color
                    r_bg = HexColor('#d1fae5')
                
                # Risk card
                risk_card = [[
                    Paragraph(f'<b>Risk {idx}: {title}</b>', ParagraphStyle('RiskTitle', fontSize=11, textColor=r_color, fontName='Helvetica-Bold')),
                    Paragraph(f'<b>{level}</b>', ParagraphStyle('Level', fontSize=10, textColor=r_color, alignment=TA_RIGHT))
                ]]
                
                risk_header = Table(risk_card, colWidths=[4.2*inch, 1.0*inch])
                risk_header.setStyle(TableStyle([
                    ('BACKGROUND', (0, 0), (-1, -1), r_bg),
                    ('ALIGN', (0, 0), (0, -1), 'LEFT'),
                    ('ALIGN', (1, 0), (1, -1), 'RIGHT'),
                    ('VALIGN', (0, 0), (-1, -1), 'MIDDLE'),
                    ('PADDING', (0, 0), (-1, -1), 8),
                    ('LINEABOVE', (0, 0), (-1, 0), 2, r_color),
                ]))
                elements.append(risk_header)
                
                elements.append(Paragraph(f'<b>Description:</b> {desc}', body_style))
                
                if mitigation and mitigation != 'N/A':
                    elements.append(Paragraph(f'<b>Mitigation:</b> {mitigation}', body_style))
                
                if impact and impact != 'N/A':
                    elements.append(Paragraph(f'<b>Potential Impact:</b> {impact}', body_style))
                
                elements.append(Spacer(1, 0.15*inch))
        
        elements.append(PageBreak())
        
        # ============================================================
        # PAGE 8: PUBLIC DATA
        # ============================================================
        print("   Page 8: Public Data")
        elements.append(Paragraph("PUBLIC DATA & MARKET INTELLIGENCE", page_title))
        elements.append(Spacer(1, 0.1*inch))
        
        pub = analysis_result.get('public_data', {})
        
        if pub and isinstance(pub, dict):
            if pub.get('github_profiles'):
                elements.append(Paragraph("üîó GitHub Developer Activity", section_heading))
                for prof in pub['github_profiles'][:3]:
                    elements.append(Paragraph(
                        f"‚Ä¢ <b>{prof.get('name', 'N/A')}</b> (@{prof.get('github_username', 'N/A')}) - "
                        f"{prof.get('public_repos', 0)} repos, {prof.get('followers', 0)} followers",
                        body_style
                    ))
                elements.append(Spacer(1, 0.1*inch))
            
            if pub.get('comprehensive_news'):
                elements.append(Paragraph("üì∞ Recent News & Press", section_heading))
                for news in pub['comprehensive_news'][:5]:
                    elements.append(Paragraph(
                        f"‚Ä¢ <b>{news.get('title', '')[:80]}</b> - {news.get('source', 'Unknown')}",body_style))
                    elements.append(Spacer(1, 0.1*inch))
            
            if pub.get('company_info'):
                elements.append(Paragraph("üè¢ Company Information", section_heading))
                ci = pub['company_info']
                if ci.get('website'):
                    elements.append(Paragraph(f"<b>Website:</b> {ci.get('website')}", body_style))
                if ci.get('employees'):
                    elements.append(Paragraph(f"<b>Employees:</b> {ci.get('employees')}", body_style))
    
            elements.append(PageBreak())
    
            # ============================================================
            # PAGE 9: FOOTER
            # ============================================================
            print("   Page 9: Footer")
            elements.append(Spacer(1, 2.0*inch))
            
            footer_style = ParagraphStyle(
                'Footer', 
                parent=styles['Normal'], 
                fontSize=8, 
                textColor=HexColor('#94a3b8'), 
                alignment=TA_CENTER
            )
            
            dt_now = datetime.now().strftime('%B %d, %Y at %I:%M %p')
            meta = analysis_result.get('analysisMetadata', {})
            
            elements.append(Paragraph(
                f"<b>Analysis Generated:</b> {dt_now}",
                footer_style
            ))
            elements.append(Paragraph(
                f"<b>AI Model:</b> {meta.get('aiModel', 'Gemini')} | <b>Documents Processed:</b> {meta.get('documentsProcessed', 'N/A')}",
                footer_style
            ))
            elements.append(Spacer(1, 0.2*inch))
            elements.append(Paragraph(
                "<i>This analysis is generated by AI and should be reviewed by investment professionals.</i>",
                ParagraphStyle('Disclaimer', fontSize=7, textColor=HexColor('#cbd5e1'), alignment=TA_CENTER)
            ))
            
            # Build PDF
            print("   Building PDF...")
            doc.build(elements)
            buffer.seek(0)
            pdf_bytes = buffer.getvalue()
            
            print(f"‚úÖ PDF built: {len(pdf_bytes)} bytes")
            
            if len(pdf_bytes) < 500:
                print(f"‚ùå PDF too small")
                return None
            
            return pdf_bytes

    except Exception as e:
        print(f"‚ùå PDF error: {str(e)}")
        import traceback
        traceback.print_exc()
        return None

# =============================================================================
# PDF HELPER FUNCTIONS (Top-level, outside main function)
# =============================================================================

def create_revenue_chart(projections):
    """Create bar chart for revenue projections"""
    from reportlab.graphics.shapes import Drawing
    from reportlab.graphics.charts.barcharts import VerticalBarChart
    from reportlab.lib.colors import HexColor
    import re
    
    if not projections or len(projections) == 0:
        return None
    
    drawing = Drawing(400, 200)
    chart = VerticalBarChart()
    chart.x = 50
    chart.y = 50
    chart.height = 125
    chart.width = 300
    
    # Colors
    accent_color = HexColor('#00d4ff')
    primary_color = HexColor('#003366')
    
    # Extract data
    years = []
    revenues = []
    for proj in projections[:5]:
        year_str = str(proj.get('year', ''))
        revenue_str = str(proj.get('revenue', '0'))
        
        # Extract year
        if year_str:
            years.append(year_str[-2:] if len(year_str) >= 2 else year_str)
        
        # Extract revenue number
        revenue_match = re.search(r'[\d.]+', revenue_str.replace(',', ''))
        if revenue_match:
            revenues.append(float(revenue_match.group()))
        else:
            revenues.append(0)
    
    if not revenues or all(r == 0 for r in revenues):
        return None
    
    chart.data = [tuple(revenues)]
    chart.categoryAxis.categoryNames = years
    chart.categoryAxis.labels.fontSize = 8
    chart.valueAxis.labels.fontSize = 8
    
    # Colors
    chart.bars[0].fillColor = accent_color
    chart.bars[0].strokeColor = primary_color
    chart.bars[0].strokeWidth = 1
    
    drawing.add(chart)
    return drawing


def create_risk_pie_chart(risks):
    """Create pie chart showing risk level distribution"""
    from reportlab.graphics.shapes import Drawing
    from reportlab.graphics.charts.piecharts import Pie
    from reportlab.lib.colors import HexColor
    
    if not risks or len(risks) == 0:
        return None
    
    drawing = Drawing(200, 200)
    pie = Pie()
    pie.x = 50
    pie.y = 50
    pie.width = 100
    pie.height = 100
    
    # Colors
    danger_color = HexColor('#ef4444')
    warning_color = HexColor('#f59e0b')
    success_color = HexColor('#22c55e')
    
    # Count risk levels
    risk_counts = {'high': 0, 'medium': 0, 'low': 0}
    for risk in risks:
        level = str(risk.get('level', 'medium')).lower()
        if level in risk_counts:
            risk_counts[level] += 1
    
    if sum(risk_counts.values()) == 0:
        return None
    
    pie.data = [risk_counts['high'], risk_counts['medium'], risk_counts['low']]
    pie.labels = ['High', 'Medium', 'Low']
    pie.slices[0].fillColor = danger_color
    pie.slices[1].fillColor = warning_color
    pie.slices[2].fillColor = success_color
    
    drawing.add(pie)
    return drawing



def synthesize_public_data_with_gemini(startup_name: str, analysis: dict) -> dict:
    """Complete public data synthesis with GitHub + SerpAPI + NewsAPI + Hunter"""
    try:
        print(f"\n{'='*70}")
        print(f"üåê FETCHING COMPREHENSIVE PUBLIC DATA for: {startup_name}")
        print(f"{'='*70}")
        
        initialize_api_usage_tracking()
        
        public_data = {
            'github_profiles': [],
            'comprehensive_news': [],
            'company_info': {},
            'funding_signals': {},
            'api_usage': {
                'github': {'used': 0, 'limit': 'unlimited', 'can_call': True},
                'serpapi': check_api_limits('serpapi'),
                'newsapi': check_api_limits('newsapi'),
                'hunter': check_api_limits('hunter')
            },
            'source': 'real_apis',
            'data_quality': {
                'github': 'real',
                'serpapi': 'real',
                'news': 'real',
                'company_info': 'real'
            }
        }
        
        # ===== 1. GitHub Profiles (FREE) =====
        print(f"\nüìã STEP 1/4: GitHub Founder Profiles")
        try:
            founders = extract_founders_from_analysis(analysis)
            if founders:
                github_result = fetch_founder_github_profiles(founders)
                public_data['github_profiles'] = github_result.get('github_profiles', [])
            else:
                print(f"   ‚ö†Ô∏è No founders extracted from analysis")
        except Exception as e:
            print(f"   ‚ùå GitHub error: {str(e)[:100]}")
        
        # ===== 2. SerpAPI Comprehensive Search =====
        print(f"\nüìã STEP 2/4: SerpAPI Comprehensive Search")
        if public_data['api_usage']['serpapi']['can_call']:
            try:
                serpapi_key = os.getenv('SERPAPI_KEY')
                if serpapi_key:
                    serp_results = search_startup_comprehensively(startup_name, serpapi_key)
                    public_data['comprehensive_news'].extend(serp_results.get('news', []))
                    
                    # Store company info from SerpAPI
                    if serp_results.get('company_info'):
                        public_data['company_info'].update(serp_results['company_info'])
                else:
                    print(f"   ‚ùå SerpAPI key not found")
            except Exception as e:
                print(f"   ‚ùå SerpAPI error: {str(e)[:100]}")
        else:
            print(f"   ‚ùå SerpAPI quota exceeded")
        
        # ===== 3. NewsAPI (Real News) =====
        print(f"\nüìã STEP 3/4: NewsAPI Recent News")
        if public_data['api_usage']['newsapi']['can_call']:
            try:
                newsapi_key = os.getenv('NEWSAPI_KEY')
                if newsapi_key:
                    print(f"   [3/4] NewsAPI - Searching for recent articles...")
                    
                    params = {
                        'q': startup_name,
                        'sortBy': 'publishedAt',
                        'language': 'en',
                        'apiKey': newsapi_key,
                        'pageSize': 5
                    }
                    
                    response = requests.get(
                        "https://newsapi.org/v2/everything",
                        params=params,
                        timeout=15
                    )
                    
                    if response.status_code == 200:
                        articles = response.json().get('articles', [])
                        
                        for article in articles[:5]:
                            public_data['comprehensive_news'].append({
                                'title': article.get('title', ''),
                                'source': article.get('source', {}).get('name', 'NewsAPI'),
                                'link': article.get('url', ''),
                                'date': article.get('publishedAt', ''),
                                'snippet': article.get('description', '')[:150],
                                'type': 'news'
                            })
                        
                        print(f"   ‚úÖ NewsAPI: Found {len(articles)} articles")
                        increment_api_usage('newsapi')
                    
                    elif response.status_code == 401:
                        print(f"   ‚ùå NewsAPI: Unauthorized (401)")
                    else:
                        print(f"   ‚ö†Ô∏è NewsAPI error: {response.status_code}")
                else:
                    print(f"   ‚ùå NewsAPI key not found")
                    
            except requests.exceptions.Timeout:
                print(f"   ‚ö†Ô∏è NewsAPI timeout")
            except Exception as e:
                print(f"   ‚ùå NewsAPI error: {str(e)[:100]}")
        else:
            print(f"   ‚ùå NewsAPI quota exceeded")
        
        # ===== 4. Hunter.io (Company Info) =====
        print(f"\nüìã STEP 4/4: Hunter.io Company Information")
        if public_data['api_usage']['hunter']['can_call']:
            try:
                hunter_key = os.getenv('HUNTER_KEY')
                if hunter_key:
                    print(f"   [4/4] Hunter.io - Fetching company details...")
                    
                    # Try to get real domain from company_info first
                    domain = public_data.get('company_info', {}).get('website', '')
                    
                    if not domain:
                        # Fallback to constructed domain
                        domain = f"{startup_name.lower().replace(' ', '')}.com"
                    
                    # Clean domain
                    domain = domain.replace('https://', '').replace('http://', '').split('/')[0]
                    
                    print(f"   Looking up: {domain}")
                    
                    params = {
                        'domain': domain,
                        'api_key': hunter_key
                    }
                    
                    response = requests.get(
                        "https://api.hunter.io/v2/domain-search",
                        params=params,
                        timeout=15
                    )
                    
                    if response.status_code == 200:
                        hunter_data = response.json().get('data', {})
                        
                        public_data['company_info'].update({
                            'website': hunter_data.get('domain', domain),
                            'employees': hunter_data.get('employees', 'N/A'),
                            'tech_stack': hunter_data.get('technologies', [])[:5],
                            'industry': hunter_data.get('industry', 'N/A'),
                            'country': hunter_data.get('country', 'N/A'),
                            'company_type': hunter_data.get('company_type', 'N/A'),
                            'founded': hunter_data.get('founded_year', 'N/A')
                        })
                        
                        print(f"   ‚úÖ Hunter.io: Got company details")
                        increment_api_usage('hunter')
                    
                    elif response.status_code == 404:
                        print(f"   ‚ö†Ô∏è Hunter.io: Domain not found ({domain})")
                    else:
                        print(f"   ‚ö†Ô∏è Hunter.io error: {response.status_code}")
                else:
                    print(f"   ‚ùå Hunter key not found")
                    
            except Exception as e:
                print(f"   ‚ùå Hunter.io error: {str(e)[:100]}")
        else:
            print(f"   ‚ùå Hunter quota exceeded")
        
        # ===== SUMMARY =====
        print(f"\n{'='*70}")
        print(f"‚úÖ PUBLIC DATA FETCH COMPLETE")
        print(f"{'='*70}")
        print(f"üìä Results:")
        print(f"   GitHub profiles: {len(public_data['github_profiles'])}")
        print(f"   News articles: {len(public_data['comprehensive_news'])}")
        print(f"   Company info: {len([k for k,v in public_data['company_info'].items() if v and v != 'N/A'])}")
        print(f"   Source: Real APIs only")
        
        return public_data
        
    except Exception as e:
        print(f"‚ùå Public data synthesis error: {str(e)}")
        import traceback
        traceback.print_exc()
        return {
            'github_profiles': [],
            'comprehensive_news': [],
            'company_info': {},
            'error': str(e),
            'source': 'real_apis'
        }


def initialize_api_usage_tracking():
    """Initialize API usage tracking in Firestore on first call"""
    try:
        usage_ref = db.collection('api_usage').document('monthly')
        usage_ref.get()  # Check if exists
        
        # If doesn't exist, create it
        current_month = datetime.now().strftime("%Y-%m")
        usage_ref.set({
            'month': current_month,
            'serpapi_calls': 0,
            'serpapi_limit': 250,
            'newsapi_calls': 0,
            'newsapi_limit': 2000,
            'hunter_calls': 0,
            'hunter_limit': 60,
            'last_updated': datetime.now().isoformat()
        }, merge=True)
        print(f"\n üìã API usage tracking initialized")
    except Exception as e:
        print(f"\n üìã API tracking error (non-critical): {str(e)}")


def check_api_limits(api_name: str) -> dict:
    """Check if API has quota remaining"""
    try:
        usage_ref = db.collection('api_usage').document('monthly')
        usage_doc = usage_ref.get()
        
        if not usage_doc.exists:
            initialize_api_usage_tracking()
            usage_doc = usage_ref.get()
        
        usage_data = usage_doc.to_dict()
        
        if api_name == 'serpapi':
            calls_used = usage_data.get('serpapi_calls', 0)
            limit = usage_data.get('serpapi_limit', 250)
            remaining = limit - calls_used
            buffer = 5  # Keep 5 calls as buffer
            
            can_call = remaining > buffer
            print(f"\n üìã  SerpAPI: {calls_used}/{limit} used, {remaining} remaining, CAN_CALL: {can_call}")
            
            return {
                'can_call': can_call,
                'used': calls_used,
                'limit': limit,
                'remaining': remaining
            }
        
        elif api_name == 'newsapi':
            calls_used = usage_data.get('newsapi_calls', 0)
            limit = usage_data.get('newsapi_limit', 2000)
            remaining = limit - calls_used
            buffer = 10  # Keep 10 for buffer
            
            can_call = remaining > buffer
            print(f"\n üìã  NewsAPI: {calls_used}/{limit} used, {remaining} remaining, CAN_CALL: {can_call}")
            
            return {
                'can_call': can_call,
                'used': calls_used,
                'limit': limit,
                'remaining': remaining
            }
        
        elif api_name == 'hunter':
            calls_used = usage_data.get('hunter_calls', 0)
            limit = usage_data.get('hunter_limit', 60)
            remaining = limit - calls_used
            buffer = 2  # Keep 2 for buffer
            
            can_call = remaining > buffer
            print(f"\n üìã  Hunter.io: {calls_used}/{limit} used, {remaining} remaining, CAN_CALL: {can_call}")
            
            return {
                'can_call': can_call,
                'used': calls_used,
                'limit': limit,
                'remaining': remaining
            }
        
        return {'can_call': False, 'error': 'Unknown API'}
        
    except Exception as e:
        print(f"\n üìã API limit check error: {str(e)}")
        return {'can_call': False, 'error': str(e)}


def increment_api_usage(api_name: str):
    """Increment API call counter after successful call"""
    try:
        usage_ref = db.collection('api_usage').document('monthly')
        
        if api_name == 'serpapi':
            usage_ref.update({'serpapi_calls': firestore.Increment(1)})
        elif api_name == 'newsapi':
            usage_ref.update({'newsapi_calls': firestore.Increment(1)})
        elif api_name == 'hunter':
            usage_ref.update({'hunter_calls': firestore.Increment(1)})
        
        print(f"\n üìã {api_name} usage incremented")
    except Exception as e:
        print(f"\n üìã Failed to update usage: {str(e)}")         


# =============================================================================
# UTILITY FUNCTIONS
# =============================================================================

def create_page_map_from_text(text: str, filename: str) -> list:
    """Create page map from text"""
    if not text or len(text) < 10:
        return []
    
    parts = text.split("---DOCUMENT_BREAK---")
    page_maps = []
    
    for i, part in enumerate(parts):
        if not part.strip():
            continue
        
        page_maps.append({
            "page_number": i + 1,
            "file_name": filename,
            "text_preview": part.strip()[:200] + "..." if len(part.strip()) > 200 else part.strip()
        })
    
    return page_maps


def get_document_type(filename: str) -> str:
    """Determine document type from filename"""
    filename = filename.lower()
    
    if any(x in filename for x in ['pitch', 'deck', 'presentation']):
        return 'pitchDeck'
    elif any(x in filename for x in ['financial', 'model', 'finance']):
        return 'financialModel'
    elif any(x in filename for x in ['founder', 'team', 'profile']):
        return 'founderProfiles'
    elif any(x in filename for x in ['market', 'research', 'analysis']):
        return 'marketResearch'
    elif any(x in filename for x in ['traction', 'metric', 'growth']):
        return 'tractionData'
    else:
        return 'document'


def get_industry_from_sector(sector: str) -> str:
    """Map sector code to industry display name"""
    sector_map = {
        'saas': 'Software as a Service',
        'fintech': 'Financial Technology',
        'healthtech': 'Healthcare & Medical Technology',
        'edtech': 'Education Technology',
        'ai': 'Artificial Intelligence & Machine Learning',
        'ecommerce': 'E-Commerce & Direct-to-Consumer',
        'mobility': 'Mobility & Transportation',
        'climate': 'Climate Technology & Sustainability',
        'consumer': 'Consumer Applications & Services',
        'other': 'Other Technology'
    }
    return sector_map.get(sector, sector.title())

    # =============================================================================
    # HEALTH CHECK ENDPOINT
    # =============================================================================
@functions_framework.http
def health_check(request: Request):
    """Health check endpoint for Cloud Function"""
    
    # Handle CORS preflight
    if request.method == 'OPTIONS':
        headers = get_cors_headers(include_content_type=False)
        return ('', 204, headers)
    
    # Only accept GET requests
    if request.method != 'GET':
        headers = get_cors_headers()
        return (
            json.dumps({"error": "Method not allowed. Use GET."}),
            405,
            headers
        )
    
    cors_headers = get_cors_headers()

    health_result = {
        "status": "healthy",
        "service": "AnalystIQ Investment Analysis",
        "processor_id": PROCESSOR_ID,
        "project": PROJECT_ID,
        "location": LOCATION,
        "version": "1.0-production",
        "timestamp": datetime.now().isoformat()
        
    }
    
    print("‚úÖ Health check passed")
    return (json.dumps(health_result), 200, cors_headers)
